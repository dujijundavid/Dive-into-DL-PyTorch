# æ¢¯åº¦ç›¸å…³é—®é¢˜ ğŸ“ˆ

> æ·±å…¥ç†è§£ PyTorch ä¸­çš„æ¢¯åº¦æœºåˆ¶ï¼Œä»æ ¹æœ¬ä¸Šè§£å†³æ¢¯åº¦çˆ†ç‚¸ã€æ¢¯åº¦æ¶ˆå¤±å’Œæ¢¯åº¦å¼‚å¸¸é—®é¢˜

## ğŸ“‹ é—®é¢˜æ¸…å•

| é—®é¢˜ç±»å‹ | æ–‡ä»¶ | ç°è±¡ | å…³é”®è¯ |
|---------|------|------|--------|
| æ¢¯åº¦çˆ†ç‚¸ | `gradient-explosion.py` | æŸå¤±å¿«é€Ÿå¢å¤§ï¼Œæ¢¯åº¦èŒƒæ•°æå¤§ | `loss -> inf`, `grad norm >> 1` |
| æ¢¯åº¦æ¶ˆå¤± | `gradient-vanishing.py` | æŸå¤±ä¸é™ï¼Œæ¢¯åº¦æ¥è¿‘0 | `grad norm << 1e-6` |
| æ¢¯åº¦ä¸º None | `gradient-none.py` | åå‘ä¼ æ’­å¤±è´¥ | `grad is None` |
| æ¢¯åº¦ä¸æ›´æ–° | `gradient-not-updating.py` | å‚æ•°å€¼ä¸å˜ | `param unchanged` |

## ğŸ”¬ ç¬¬ä¸€æ€§åŸç†åˆ†æ

### æ¢¯åº¦çš„æœ¬è´¨
```python
# æ¢¯åº¦ = æŸå¤±å‡½æ•°å¯¹å‚æ•°çš„åå¯¼æ•°
âˆ‚L/âˆ‚Î¸ = lim(hâ†’0) [L(Î¸+h) - L(Î¸)] / h
```

### åå‘ä¼ æ’­é“¾å¼æ³•åˆ™
```python
# å¤åˆå‡½æ•°æ±‚å¯¼
âˆ‚L/âˆ‚Î¸â‚ = (âˆ‚L/âˆ‚zâ‚‚) Ã— (âˆ‚zâ‚‚/âˆ‚zâ‚) Ã— (âˆ‚zâ‚/âˆ‚Î¸â‚)
```

å½“é“¾å¼ç›¸ä¹˜çš„é¡¹è¿‡å¤§æˆ–è¿‡å°æ—¶ï¼Œä¼šå¯¼è‡´æ¢¯åº¦çˆ†ç‚¸æˆ–æ¶ˆå¤±ã€‚

## ğŸš¨ å¸¸è§é”™è¯¯æ¨¡å¼

### 1. æ•°æ®è§„æ¨¡é—®é¢˜
- **è¾“å…¥æ•°æ®é‡çº§è¿‡å¤§** â†’ æ¿€æ´»å€¼è¿‡å¤§ â†’ æ¢¯åº¦çˆ†ç‚¸
- **ç›®æ ‡å€¼é‡çº§è¿‡å¤§** â†’ æŸå¤±å€¼è¿‡å¤§ â†’ æ¢¯åº¦çˆ†ç‚¸

### 2. å­¦ä¹ ç‡é—®é¢˜
- **å­¦ä¹ ç‡è¿‡å¤§** â†’ å‚æ•°æ›´æ–°æ­¥é•¿è¿‡å¤§ â†’ æ¢¯åº¦çˆ†ç‚¸
- **å­¦ä¹ ç‡è¿‡å°** â†’ æ”¶æ•›ææ…¢ï¼Œçœ‹ä¼¼æ¢¯åº¦æ¶ˆå¤±

### 3. ç½‘ç»œæ¶æ„é—®é¢˜
- **ç½‘ç»œè¿‡æ·±** â†’ æ¢¯åº¦åœ¨ä¼ æ’­ä¸­è¡°å‡ â†’ æ¢¯åº¦æ¶ˆå¤±
- **æ¿€æ´»å‡½æ•°é€‰æ‹©ä¸å½“** â†’ sigmoid é¥±å’ŒåŒºåŸŸ â†’ æ¢¯åº¦æ¶ˆå¤±

## ğŸ› ï¸ ç³»ç»Ÿæ€§è§£å†³æ–¹æ¡ˆ

### è¯Šæ–­å·¥å…·
```python
def diagnose_gradients(model):
    """æ¢¯åº¦è¯Šæ–­å·¥å…·å‡½æ•°"""
    grad_norms = []
    for name, param in model.named_parameters():
        if param.grad is not None:
            grad_norm = param.grad.data.norm(2).item()
            grad_norms.append((name, grad_norm))
            print(f"{name}: {grad_norm:.6f}")
    return grad_norms
```

### ä¿®å¤ç­–ç•¥
1. **æ•°æ®é¢„å¤„ç†**ï¼šæ ‡å‡†åŒ–è¾“å…¥å’Œç›®æ ‡
2. **æ¢¯åº¦è£å‰ª**ï¼š`torch.nn.utils.clip_grad_norm_()`
3. **å­¦ä¹ ç‡è°ƒæ•´**ï¼šä½¿ç”¨å­¦ä¹ ç‡è°ƒåº¦å™¨
4. **ç½‘ç»œè®¾è®¡**ï¼šæ®‹å·®è¿æ¥ã€æ‰¹æ ‡å‡†åŒ–

## ğŸ“š å‚è€ƒèµ„æ–™

- [Understanding the difficulty of training deep feedforward neural networks](http://proceedings.mlr.press/v9/glorot10a.html)
- [On the difficulty of training recurrent neural networks](http://proceedings.mlr.press/v28/pascanu13.html) 