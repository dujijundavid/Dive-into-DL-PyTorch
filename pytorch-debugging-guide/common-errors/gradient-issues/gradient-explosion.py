"""
æ¢¯åº¦çˆ†ç‚¸é—®é¢˜åˆ†æä¸è§£å†³æ–¹æ¡ˆ
=========================

é—®é¢˜ç°è±¡ï¼š
- æŸå¤±å€¼å¿«é€Ÿå¢é•¿åˆ° inf
- æ¢¯åº¦èŒƒæ•°æ€¥å‰§å¢å¤§
- æ¨¡å‹å‚æ•°æ›´æ–°è¿‡äºå‰§çƒˆ

ç¬¬ä¸€æ€§åŸç†åˆ†æï¼š
- é«˜å­¦ä¹ ç‡ Ã— å¤§æ•°æ®é‡çº§ Ã— å¤§ç›®æ ‡å€¼ = æ¢¯åº¦çˆ†ç‚¸
- é“¾å¼æ³•åˆ™ï¼šâˆ‚L/âˆ‚Î¸ = (âˆ‚L/âˆ‚output) Ã— (âˆ‚output/âˆ‚Î¸)
- å½“ âˆ‚L/âˆ‚output å¾ˆå¤§æ—¶ï¼Œæ•´ä¸ªæ¢¯åº¦ä¼šè¢«æ”¾å¤§
"""

import torch
import torch.nn as nn
import torch.optim as optim
import matplotlib.pyplot as plt
from typing import List, Tuple

# è®¾ç½®éšæœºç§å­ç¡®ä¿ç»“æœå¯å¤ç°
torch.manual_seed(0)

# ================== æœ‰é—®é¢˜çš„ä»£ç  ==================

class SimpleMLP(nn.Module):
    """ç®€å•çš„å¤šå±‚æ„ŸçŸ¥æœº"""
    def __init__(self):
        super().__init__()
        self.fc = nn.Sequential(
            nn.Linear(100, 256),
            nn.ReLU(),
            nn.Linear(256, 1)
        )

    def forward(self, x):
        return self.fc(x)

def problematic_training():
    """å±•ç¤ºæ¢¯åº¦çˆ†ç‚¸çš„é—®é¢˜ä»£ç """
    print("=== æ¢¯åº¦çˆ†ç‚¸ç¤ºä¾‹ ===")
    
    model = SimpleMLP()
    optimizer = optim.SGD(model.parameters(), lr=1.0)  # ğŸš¨ å­¦ä¹ ç‡è¿‡å¤§
    criterion = nn.MSELoss()

    # ğŸš¨ è¾“å…¥å’Œç›®æ ‡å€¼é‡çº§è¿‡å¤§
    x = torch.randn(64, 100) * 100      # è¾“å…¥æ”¾å¤§100å€
    target = torch.randn(64, 1) * 1000  # ç›®æ ‡æ”¾å¤§1000å€

    losses = []
    grad_norms = []
    
    for i in range(20):  # å‡å°‘è¿­ä»£æ¬¡æ•°é¿å…æº¢å‡º
        optimizer.zero_grad()
        output = model(x)
        loss = criterion(output, target)
        loss.backward()
        
        # è®¡ç®—æ¢¯åº¦èŒƒæ•°
        total_grad_norm = sum(p.grad.data.norm(2).item() for p in model.parameters())
        
        print(f"Step {i:2d}, Loss: {loss.item():10.4f}, Grad Norm: {total_grad_norm:8.2f}")
        
        losses.append(loss.item())
        grad_norms.append(total_grad_norm)
        
        # æ£€æŸ¥æ˜¯å¦å·²ç»çˆ†ç‚¸
        if loss.item() > 1e6 or total_grad_norm > 1e6:
            print("âš ï¸  æ¢¯åº¦çˆ†ç‚¸æ£€æµ‹åˆ°ï¼åœæ­¢è®­ç»ƒ")
            break
            
        optimizer.step()
    
    return losses, grad_norms

# ================== ä¿®å¤æ–¹æ¡ˆ ==================

def diagnose_gradients(model):
    """è¯Šæ–­æ¯å±‚çš„æ¢¯åº¦æƒ…å†µ"""
    print("\n=== æ¢¯åº¦è¯Šæ–­ ===")
    for name, param in model.named_parameters():
        if param.grad is not None:
            grad_norm = param.grad.data.norm(2).item()
            param_norm = param.data.norm(2).item()
            print(f"{name:20s}: grad_norm={grad_norm:8.4f}, param_norm={param_norm:8.4f}")

def fixed_training():
    """ä¿®å¤åçš„è®­ç»ƒä»£ç """
    print("\n=== ä¿®å¤åçš„è®­ç»ƒ ===")
    
    model = SimpleMLP()
    
    # ä¿®å¤1: é™ä½å­¦ä¹ ç‡
    optimizer = optim.SGD(model.parameters(), lr=0.001)  # âœ… åˆé€‚çš„å­¦ä¹ ç‡
    criterion = nn.MSELoss()

    # ä¿®å¤2: æ•°æ®æ ‡å‡†åŒ–
    x = torch.randn(64, 100)      # âœ… æ ‡å‡†åŒ–è¾“å…¥
    target = torch.randn(64, 1)   # âœ… æ ‡å‡†åŒ–ç›®æ ‡
    
    # ä¿®å¤3: æ¢¯åº¦è£å‰ª
    max_grad_norm = 1.0
    
    losses = []
    grad_norms = []
    
    for i in range(100):
        optimizer.zero_grad()
        output = model(x)
        loss = criterion(output, target)
        loss.backward()
        
        # ä¿®å¤3: åº”ç”¨æ¢¯åº¦è£å‰ª
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)
        
        total_grad_norm = sum(p.grad.data.norm(2).item() for p in model.parameters())
        
        if i % 10 == 0:
            print(f"Step {i:2d}, Loss: {loss.item():8.4f}, Grad Norm: {total_grad_norm:6.4f}")
        
        losses.append(loss.item())
        grad_norms.append(total_grad_norm)
        
        optimizer.step()
    
    return losses, grad_norms

def advanced_fixed_training():
    """é«˜çº§ä¿®å¤æ–¹æ¡ˆï¼šè‡ªé€‚åº”å­¦ä¹ ç‡ + æ‰¹æ ‡å‡†åŒ–"""
    print("\n=== é«˜çº§ä¿®å¤æ–¹æ¡ˆ ===")
    
    # ä¿®å¤4: æ”¹è¿›ç½‘ç»œæ¶æ„
    class ImprovedMLP(nn.Module):
        def __init__(self):
            super().__init__()
            self.fc = nn.Sequential(
                nn.Linear(100, 256),
                nn.BatchNorm1d(256),  # âœ… æ‰¹æ ‡å‡†åŒ–
                nn.ReLU(),
                nn.Dropout(0.2),      # âœ… æ­£åˆ™åŒ–
                nn.Linear(256, 128),
                nn.BatchNorm1d(128),
                nn.ReLU(),
                nn.Dropout(0.2),
                nn.Linear(128, 1)
            )
        
        def forward(self, x):
            return self.fc(x)
    
    model = ImprovedMLP()
    
    # ä¿®å¤5: ä½¿ç”¨è‡ªé€‚åº”ä¼˜åŒ–å™¨
    optimizer = optim.Adam(model.parameters(), lr=0.001)  # âœ… Adamä¼˜åŒ–å™¨
    criterion = nn.MSELoss()
    
    # ä¿®å¤6: å­¦ä¹ ç‡è°ƒåº¦
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10, factor=0.5)
    
    # åŸå§‹æ•°æ®ï¼ˆæ•…æ„ä¿æŒå¤§é‡çº§æµ‹è¯•ä¿®å¤æ•ˆæœï¼‰
    x = torch.randn(64, 100) * 10   # é€‚ä¸­çš„æ”¾å¤§
    target = torch.randn(64, 1) * 10
    
    losses = []
    grad_norms = []
    
    for i in range(100):
        optimizer.zero_grad()
        output = model(x)
        loss = criterion(output, target)
        loss.backward()
        
        # æ¢¯åº¦è£å‰ª
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        
        total_grad_norm = sum(p.grad.data.norm(2).item() for p in model.parameters())
        
        if i % 10 == 0:
            print(f"Step {i:2d}, Loss: {loss.item():8.4f}, Grad Norm: {total_grad_norm:6.4f}, LR: {optimizer.param_groups[0]['lr']:.6f}")
        
        losses.append(loss.item())
        grad_norms.append(total_grad_norm)
        
        optimizer.step()
        scheduler.step(loss)  # æ ¹æ®æŸå¤±è°ƒæ•´å­¦ä¹ ç‡
    
    return losses, grad_norms

def plot_comparison(problem_losses, problem_grads, fixed_losses, fixed_grads, advanced_losses, advanced_grads):
    """å¯è§†åŒ–æ¯”è¾ƒç»“æœ"""
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
    
    # æŸå¤±å¯¹æ¯”
    ax1.plot(problem_losses[:len(problem_losses)], 'r-', label='é—®é¢˜ä»£ç ', linewidth=2)
    ax1.plot(fixed_losses, 'g-', label='åŸºç¡€ä¿®å¤', linewidth=2)
    ax1.plot(advanced_losses, 'b-', label='é«˜çº§ä¿®å¤', linewidth=2)
    ax1.set_xlabel('è®­ç»ƒæ­¥æ•°')
    ax1.set_ylabel('æŸå¤±å€¼')
    ax1.set_title('æŸå¤±å€¼å¯¹æ¯”')
    ax1.legend()
    ax1.set_yscale('log')
    
    # æ¢¯åº¦èŒƒæ•°å¯¹æ¯”
    ax2.plot(problem_grads[:len(problem_grads)], 'r-', label='é—®é¢˜ä»£ç ', linewidth=2)
    ax2.plot(fixed_grads, 'g-', label='åŸºç¡€ä¿®å¤', linewidth=2)
    ax2.plot(advanced_grads, 'b-', label='é«˜çº§ä¿®å¤', linewidth=2)
    ax2.set_xlabel('è®­ç»ƒæ­¥æ•°')
    ax2.set_ylabel('æ¢¯åº¦èŒƒæ•°')
    ax2.set_title('æ¢¯åº¦èŒƒæ•°å¯¹æ¯”')
    ax2.legend()
    ax2.set_yscale('log')
    
    plt.tight_layout()
    plt.savefig('gradient_explosion_comparison.png', dpi=300, bbox_inches='tight')
    plt.show()

def main():
    """ä¸»å‡½æ•°ï¼šæ¼”ç¤ºæ¢¯åº¦çˆ†ç‚¸é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ"""
    print("ğŸ”¬ PyTorch æ¢¯åº¦çˆ†ç‚¸åˆ†æä¸è§£å†³æ–¹æ¡ˆ")
    print("=" * 50)
    
    # 1. å±•ç¤ºé—®é¢˜
    problem_losses, problem_grads = problematic_training()
    
    # 2. åŸºç¡€ä¿®å¤
    fixed_losses, fixed_grads = fixed_training()
    
    # 3. é«˜çº§ä¿®å¤
    advanced_losses, advanced_grads = advanced_fixed_training()
    
    # 4. å¯è§†åŒ–å¯¹æ¯”
    plot_comparison(problem_losses, problem_grads, fixed_losses, fixed_grads, advanced_losses, advanced_grads)
    
    print("\nâœ… æ€»ç»“ï¼š")
    print("1. é—®é¢˜æ ¹æºï¼šé«˜å­¦ä¹ ç‡ + å¤§æ•°æ®é‡çº§ + ä¸åˆé€‚çš„ç½‘ç»œæ¶æ„")
    print("2. åŸºç¡€è§£å†³ï¼šé™ä½å­¦ä¹ ç‡ + æ•°æ®æ ‡å‡†åŒ– + æ¢¯åº¦è£å‰ª")
    print("3. é«˜çº§è§£å†³ï¼šæ”¹è¿›æ¶æ„ + è‡ªé€‚åº”ä¼˜åŒ–å™¨ + å­¦ä¹ ç‡è°ƒåº¦")
    print("4. å…³é”®ç›‘æ§ï¼šæ¢¯åº¦èŒƒæ•°ã€æŸå¤±å€¼å˜åŒ–è¶‹åŠ¿")

if __name__ == "__main__":
    main() 