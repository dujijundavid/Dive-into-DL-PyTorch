"""
PyTorch æ·±åº¦å­¦ä¹ åŸºç¡€æ ¸å¿ƒæŠ€æœ¯ä¸æ·±åº¦ç†è§£
-----------------------------------
ã€æ–‡ä»¶è¯´æ˜ã€‘
æœ¬æ–‡ä»¶ç³»ç»Ÿæ¢³ç†äº†æ·±åº¦å­¦ä¹ çš„åŸºç¡€æ ¸å¿ƒæŠ€æœ¯ï¼ŒåŒ…æ‹¬ï¼š
- çº¿æ€§å›å½’ä¸æ¢¯åº¦ä¸‹é™ï¼šæœºå™¨å­¦ä¹ çš„èµ·ç‚¹
- å¤šå±‚æ„ŸçŸ¥æœºä¸éçº¿æ€§å˜æ¢ï¼šæ·±åº¦å­¦ä¹ çš„åŸºçŸ³
- æ­£åˆ™åŒ–æŠ€æœ¯ï¼šé˜²æ­¢è¿‡æ‹Ÿåˆçš„æ ¸å¿ƒæ–¹æ³•
- ç°ä»£è®­ç»ƒæŠ€å·§ï¼šæ‰¹é‡å¤„ç†ã€ä¼˜åŒ–å™¨é€‰æ‹©ã€å­¦ä¹ ç‡è°ƒåº¦
- å®é™…å·¥ç¨‹å®è·µï¼šæ¨¡å‹è°ƒè¯•ã€æ€§èƒ½åˆ†æã€éƒ¨ç½²ä¼˜åŒ–

ã€ç¬¬ä¸€æ€§åŸç†æ€è€ƒã€‘
1. ä¸ºä»€ä¹ˆéœ€è¦æ·±åº¦å­¦ä¹ ï¼Ÿ
   - ä¼ ç»Ÿæœºå™¨å­¦ä¹ éœ€è¦æ‰‹å·¥ç‰¹å¾å·¥ç¨‹
   - æ·±åº¦ç½‘ç»œèƒ½è‡ªåŠ¨å­¦ä¹ å±‚æ¬¡åŒ–ç‰¹å¾è¡¨ç¤º
   - éçº¿æ€§å˜æ¢çš„å¤åˆå®ç°å¤æ‚å‡½æ•°é€¼è¿‘

2. æ¢¯åº¦ä¸‹é™ä¸ºä»€ä¹ˆæœ‰æ•ˆï¼Ÿ
   - æŸå¤±å‡½æ•°çš„æ¢¯åº¦æŒ‡å‘æœ€é™¡å¢é•¿æ–¹å‘
   - è´Ÿæ¢¯åº¦æ–¹å‘æ˜¯å±€éƒ¨æœ€ä¼˜ä¸‹é™è·¯å¾„
   - é€šè¿‡è¿­ä»£é€¼è¿‘å…¨å±€æˆ–å±€éƒ¨æœ€ä¼˜è§£

3. ä¸ºä»€ä¹ˆéœ€è¦æ­£åˆ™åŒ–ï¼Ÿ
   - æœ‰é™æ•°æ®å®¹æ˜“å¯¼è‡´è¿‡æ‹Ÿåˆ
   - æ­£åˆ™åŒ–å¢åŠ å…ˆéªŒçº¦æŸï¼Œæå‡æ³›åŒ–
   - å¹³è¡¡æ¨¡å‹å¤æ‚åº¦ä¸æ‹Ÿåˆèƒ½åŠ›

ã€è‹æ ¼æ‹‰åº•å¼æé—®ä¸éªŒè¯ã€‘
1. æ›´æ·±çš„ç½‘ç»œæ€»æ˜¯æ›´å¥½å—ï¼Ÿ
   - é—®é¢˜ï¼šæ·±åº¦ä¸æ€§èƒ½çš„å…³ç³»æ˜¯ä»€ä¹ˆï¼Ÿ
   - éªŒè¯ï¼šé€šè¿‡ä¸åŒæ·±åº¦çš„MLPå¯¹æ¯”
   - ç»“è®ºï¼šéœ€è¦åˆé€‚çš„æ•°æ®é‡å’Œæ­£åˆ™åŒ–

2. æ¿€æ´»å‡½æ•°çš„é€‰æ‹©æœ‰ä½•å½±å“ï¼Ÿ
   - é—®é¢˜ï¼šReLUä¸ºä»€ä¹ˆæ¯”Sigmoidæ›´æµè¡Œï¼Ÿ
   - éªŒè¯ï¼šé€šè¿‡æ¢¯åº¦æµåŠ¨å’Œè®­ç»ƒé€Ÿåº¦å¯¹æ¯”
   - ç»“è®ºï¼šReLUè§£å†³æ¢¯åº¦æ¶ˆå¤±ï¼Œè®­ç»ƒæ›´ç¨³å®š

ã€è´¹æ›¼å­¦ä¹ æ³•è®²è§£ã€‘
1. æ¦‚å¿µè§£é‡Š
   - ç”¨å †ç§¯æœ¨ç±»æ¯”ç¥ç»ç½‘ç»œçš„å±‚æ¬¡ç»“æ„
   - ç”¨å­¦ä¹ è¿‡ç¨‹ç±»æ¯”æ¢¯åº¦ä¸‹é™ä¼˜åŒ–
   - å¼ºè°ƒæ·±åº¦å­¦ä¹ åœ¨å„é¢†åŸŸçš„é‡è¦åº”ç”¨

2. å®ä¾‹æ•™å­¦
   - ä»ç®€å•çš„çº¿æ€§å›å½’å¼€å§‹
   - é€æ­¥æ„å»ºå¤šå±‚æ„ŸçŸ¥æœº
   - é€šè¿‡å¯è§†åŒ–ç†è§£ç‰¹å¾å­¦ä¹ è¿‡ç¨‹

ã€è®¾è®¡æ„ä¹‰ä¸å·¥ç¨‹ä»·å€¼ã€‘
- æ·±åº¦å­¦ä¹ æ˜¯ç°ä»£AIçš„åŸºç¡€ï¼Œå½±å“äº†è®¡ç®—æœºè§†è§‰ã€è‡ªç„¶è¯­è¨€å¤„ç†ç­‰é¢†åŸŸ
- ç†è§£åŸºç¡€åŸç†å¯¹æŒæ¡æ›´å¤æ‚æ¨¡å‹è‡³å…³é‡è¦
- å·¥ç¨‹å®è·µæŠ€å·§ç›´æ¥å½±å“æ¨¡å‹æ€§èƒ½å’Œéƒ¨ç½²æ•ˆæœ

å¯è¿è¡Œæ¡ˆä¾‹ï¼š
"""

import torch
from torch import nn
import numpy as np
import matplotlib.pyplot as plt
from torch.utils.data import DataLoader, TensorDataset
import time

if __name__ == "__main__":
    print("========== PyTorchæ·±åº¦å­¦ä¹ åŸºç¡€æ ¸å¿ƒæŠ€æœ¯æ¼”ç¤º ==========")
    
    # 1. ç¬¬ä¸€æ€§åŸç†ï¼šä»çº¿æ€§å›å½’ç†è§£æ¢¯åº¦ä¸‹é™
    print("\n========== ç¬¬ä¸€æ€§åŸç†ï¼šæ¢¯åº¦ä¸‹é™æœ¬è´¨ ==========")
    
    def gradient_descent_visualization():
        """
        å¯è§†åŒ–æ¢¯åº¦ä¸‹é™è¿‡ç¨‹ï¼Œç†è§£ä¼˜åŒ–çš„æœ¬è´¨
        å±•ç¤ºå‚æ•°æ›´æ–°è½¨è¿¹å’ŒæŸå¤±å˜åŒ–
        """
        # åˆ›å»ºç®€å•çš„äºŒæ¬¡å‡½æ•°ä½œä¸ºæŸå¤±å‡½æ•°
        def loss_function(w):
            return (w - 2)**2 + 1
        
        def loss_gradient(w):
            return 2 * (w - 2)
        
        # æ¢¯åº¦ä¸‹é™è¿­ä»£
        w_history = []
        loss_history = []
        
        w = torch.tensor(-2.0, requires_grad=True)  # åˆå§‹å‚æ•°
        lr = 0.1  # å­¦ä¹ ç‡
        
        print("æ¢¯åº¦ä¸‹é™è¿­ä»£è¿‡ç¨‹ï¼š")
        for i in range(20):
            loss = loss_function(w)
            loss_history.append(loss.item())
            w_history.append(w.item())
            
            if i % 5 == 0:
                print(f"è¿­ä»£{i}: w={w.item():.3f}, loss={loss.item():.3f}")
            
            # è®¡ç®—æ¢¯åº¦å¹¶æ›´æ–°å‚æ•°
            loss.backward()
            with torch.no_grad():
                w -= lr * w.grad
                w.grad.zero_()
        
        print(f"æœ€ç»ˆç»“æœ: w={w.item():.3f}, ç†è®ºæœ€ä¼˜å€¼: w=2.0")
        return w_history, loss_history
    
    w_hist, loss_hist = gradient_descent_visualization()
    
    # 2. çº¿æ€§å›å½’ï¼šæ·±åº¦å­¦ä¹ çš„èµ·ç‚¹
    print("\n========== çº¿æ€§å›å½’ï¼šä»ç†è®ºåˆ°å®è·µ ==========")
    
    class LinearRegressionDetailed(nn.Module):
        """
        è¯¦ç»†çš„çº¿æ€§å›å½’å®ç°ï¼ŒåŒ…å«å‚æ•°åˆå§‹åŒ–å’Œå‰å‘ä¼ æ’­é€»è¾‘
        åŸç†ï¼šé€šè¿‡æœ€å°åŒ–å‡æ–¹è¯¯å·®å­¦ä¹ è¾“å…¥è¾“å‡ºçš„çº¿æ€§å…³ç³»
        """
        def __init__(self, in_features, out_features):
            super().__init__()
            self.linear = nn.Linear(in_features, out_features)
            # è‡ªå®šä¹‰åˆå§‹åŒ–
            nn.init.normal_(self.linear.weight, mean=0, std=0.01)
            nn.init.constant_(self.linear.bias, 0)
            
        def forward(self, x):
            return self.linear(x)
    
    def linear_regression_demo():
        """çº¿æ€§å›å½’å®Œæ•´æ¼”ç¤ºï¼šæ•°æ®ç”Ÿæˆâ†’æ¨¡å‹è®­ç»ƒâ†’ç»“æœåˆ†æ"""
        # ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
        torch.manual_seed(42)
        n_samples = 1000
        n_features = 2
        
        # çœŸå®å‚æ•°
        true_w = torch.tensor([[2.0], [-3.5]])
        true_b = 4.2
        
        # ç”Ÿæˆæ•°æ®ï¼šy = Xw + b + noise
        X = torch.randn(n_samples, n_features)
        noise = torch.randn(n_samples, 1) * 0.1
        y = X @ true_w + true_b + noise
        
        print(f"æ•°æ®å½¢çŠ¶: X={X.shape}, y={y.shape}")
        print(f"çœŸå®å‚æ•°: w={true_w.flatten().tolist()}, b={true_b}")
        
        # åˆ›å»ºæ¨¡å‹
        model = LinearRegressionDetailed(n_features, 1)
        criterion = nn.MSELoss()
        optimizer = torch.optim.SGD(model.parameters(), lr=0.01)
        
        # è®­ç»ƒè¿‡ç¨‹
        losses = []
        for epoch in range(1000):
            # å‰å‘ä¼ æ’­
            y_pred = model(X)
            loss = criterion(y_pred, y)
            
            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            losses.append(loss.item())
            
            if epoch % 200 == 0:
                print(f"Epoch {epoch}: Loss = {loss.item():.6f}")
        
        # ç»“æœåˆ†æ
        learned_w = model.linear.weight.data
        learned_b = model.linear.bias.item()
        
        print(f"å­¦ä¹ åˆ°çš„å‚æ•°: w={learned_w.flatten().tolist()}, b={learned_b:.3f}")
        print(f"å‚æ•°è¯¯å·®: w_error={torch.norm(learned_w.flatten() - true_w.flatten()).item():.6f}")
        
        return model, losses
    
    lr_model, lr_losses = linear_regression_demo()
    
    # 3. è‹æ ¼æ‹‰åº•å¼æé—®ï¼šæ¿€æ´»å‡½æ•°çš„é‡è¦æ€§
    print("\n========== è‹æ ¼æ‹‰åº•å¼æé—®ï¼šä¸ºä»€ä¹ˆéœ€è¦æ¿€æ´»å‡½æ•°ï¼Ÿ ==========")
    
    def activation_function_comparison():
        """
        å¯¹æ¯”ä¸åŒæ¿€æ´»å‡½æ•°çš„ç‰¹æ€§å’Œå½±å“
        è¯æ˜éçº¿æ€§æ¿€æ´»å‡½æ•°çš„å¿…è¦æ€§
        """
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        x = torch.linspace(-5, 5, 1000)
        
        # ä¸åŒæ¿€æ´»å‡½æ•°
        activations = {
            'ReLU': torch.relu,
            'Sigmoid': torch.sigmoid,
            'Tanh': torch.tanh,
            'LeakyReLU': lambda x: nn.functional.leaky_relu(x, 0.1),
            'ELU': nn.functional.elu
        }
        
        print("æ¿€æ´»å‡½æ•°ç‰¹æ€§åˆ†æï¼š")
        for name, func in activations.items():
            y = func(x)
            
            # è®¡ç®—æ¢¯åº¦
            x_grad = x.clone().requires_grad_()
            y_grad = func(x_grad)
            y_grad.sum().backward()
            gradient_norm = x_grad.grad.norm().item()
            
            print(f"{name}: è¾“å‡ºèŒƒå›´=[{y.min().item():.3f}, {y.max().item():.3f}], "
                  f"æ¢¯åº¦èŒƒæ•°={gradient_norm:.3f}")
        
        # ç½‘ç»œæ·±åº¦å¯¹æ¿€æ´»å‡½æ•°çš„å½±å“
        def test_deep_network_activation(activation_func, depth=10):
            """æµ‹è¯•æ·±å±‚ç½‘ç»œä¸­æ¿€æ´»å‡½æ•°çš„è¡¨ç°"""
            layers = []
            for i in range(depth):
                layers.extend([
                    nn.Linear(100, 100),
                    nn.BatchNorm1d(100) if i > 0 else nn.Identity(),
                    activation_func()
                ])
            
            network = nn.Sequential(*layers, nn.Linear(100, 1))
            
            # æµ‹è¯•æ¢¯åº¦æµåŠ¨
            x = torch.randn(32, 100)
            y = network(x)
            loss = y.sum()
            loss.backward()
            
            # ç»Ÿè®¡æ¢¯åº¦ä¿¡æ¯
            grad_stats = []
            for param in network.parameters():
                if param.grad is not None:
                    grad_stats.append(param.grad.norm().item())
            
            return np.mean(grad_stats), np.std(grad_stats)
        
        print("\næ·±å±‚ç½‘ç»œä¸­æ¿€æ´»å‡½æ•°çš„æ¢¯åº¦æµåŠ¨ï¼š")
        activation_classes = {
            'ReLU': nn.ReLU,
            'Sigmoid': nn.Sigmoid,
            'Tanh': nn.Tanh
        }
        
        for name, act_class in activation_classes.items():
            mean_grad, std_grad = test_deep_network_activation(act_class)
            print(f"{name}: å¹³å‡æ¢¯åº¦èŒƒæ•°={mean_grad:.6f}, æ ‡å‡†å·®={std_grad:.6f}")
    
    activation_function_comparison()
    
    # 4. å¤šå±‚æ„ŸçŸ¥æœºï¼šæ·±åº¦å­¦ä¹ çš„åŸºçŸ³
    print("\n========== å¤šå±‚æ„ŸçŸ¥æœºï¼šæ·±åº¦å­¦ä¹ åŸºçŸ³ ==========")
    
    class MLPDetailed(nn.Module):
        """
        è¯¦ç»†çš„å¤šå±‚æ„ŸçŸ¥æœºå®ç°ï¼ŒåŒ…å«å¤šç§é…ç½®é€‰é¡¹
        å±•ç¤ºç½‘ç»œè®¾è®¡çš„å…³é”®è€ƒè™‘å› ç´ 
        """
        def __init__(self, input_dim, hidden_dims, output_dim, 
                     activation='relu', dropout_rate=0.0, batch_norm=False):
            super().__init__()
            
            self.layers = nn.ModuleList()
            prev_dim = input_dim
            
            # æ„å»ºéšè—å±‚
            for hidden_dim in hidden_dims:
                self.layers.append(nn.Linear(prev_dim, hidden_dim))
                
                if batch_norm:
                    self.layers.append(nn.BatchNorm1d(hidden_dim))
                
                # æ¿€æ´»å‡½æ•°
                if activation == 'relu':
                    self.layers.append(nn.ReLU())
                elif activation == 'tanh':
                    self.layers.append(nn.Tanh())
                elif activation == 'sigmoid':
                    self.layers.append(nn.Sigmoid())
                
                if dropout_rate > 0:
                    self.layers.append(nn.Dropout(dropout_rate))
                
                prev_dim = hidden_dim
            
            # è¾“å‡ºå±‚
            self.layers.append(nn.Linear(prev_dim, output_dim))
        
        def forward(self, x):
            for layer in self.layers:
                x = layer(x)
            return x
    
    def mlp_architecture_comparison():
        """
        å¯¹æ¯”ä¸åŒMLPæ¶æ„çš„æ€§èƒ½
        éªŒè¯ç½‘ç»œæ·±åº¦ã€å®½åº¦ã€æ­£åˆ™åŒ–çš„å½±å“
        """
        # ç”Ÿæˆåˆ†ç±»æ•°æ®
        from sklearn.datasets import make_classification
        X, y = make_classification(n_samples=5000, n_features=20, n_classes=3, 
                                 n_informative=15, random_state=42)
        X = torch.FloatTensor(X)
        y = torch.LongTensor(y)
        
        # æ•°æ®åˆ’åˆ†
        train_size = int(0.8 * len(X))
        X_train, X_test = X[:train_size], X[train_size:]
        y_train, y_test = y[:train_size], y[train_size:]
        
        # ä¸åŒæ¶æ„é…ç½®
        architectures = {
            'Shallow': [64],
            'Deep_Narrow': [32, 32, 32, 32],
            'Deep_Wide': [128, 128, 64],
            'Deep_BN': [128, 128, 64],  # å¸¦æ‰¹å½’ä¸€åŒ–
            'Deep_Dropout': [128, 128, 64]  # å¸¦Dropout
        }
        
        results = {}
        
        for arch_name, hidden_dims in architectures.items():
            print(f"\nè®­ç»ƒæ¶æ„: {arch_name}")
            
            # åˆ›å»ºæ¨¡å‹
            batch_norm = 'BN' in arch_name
            dropout_rate = 0.3 if 'Dropout' in arch_name else 0.0
            
            model = MLPDetailed(20, hidden_dims, 3, 
                              batch_norm=batch_norm, 
                              dropout_rate=dropout_rate)
            
            criterion = nn.CrossEntropyLoss()
            optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
            
            # è®­ç»ƒ
            model.train()
            train_losses = []
            
            for epoch in range(200):
                optimizer.zero_grad()
                outputs = model(X_train)
                loss = criterion(outputs, y_train)
                loss.backward()
                optimizer.step()
                
                train_losses.append(loss.item())
            
            # æµ‹è¯•
            model.eval()
            with torch.no_grad():
                test_outputs = model(X_test)
                test_loss = criterion(test_outputs, y_test).item()
                test_acc = (test_outputs.argmax(dim=1) == y_test).float().mean().item()
            
            # è®¡ç®—å‚æ•°æ•°é‡
            param_count = sum(p.numel() for p in model.parameters())
            
            results[arch_name] = {
                'test_loss': test_loss,
                'test_acc': test_acc,
                'param_count': param_count,
                'final_train_loss': train_losses[-1]
            }
            
            print(f"å‚æ•°æ•°é‡: {param_count}")
            print(f"æœ€ç»ˆè®­ç»ƒæŸå¤±: {train_losses[-1]:.4f}")
            print(f"æµ‹è¯•æŸå¤±: {test_loss:.4f}")
            print(f"æµ‹è¯•å‡†ç¡®ç‡: {test_acc:.3f}")
        
        return results
    
    arch_results = mlp_architecture_comparison()
    
    # 5. æ­£åˆ™åŒ–æŠ€æœ¯æ·±åº¦è§£æ
    print("\n========== æ­£åˆ™åŒ–æŠ€æœ¯ï¼šé˜²æ­¢è¿‡æ‹Ÿåˆçš„è‰ºæœ¯ ==========")
    
    def regularization_comprehensive_demo():
        """
        ç»¼åˆæ¼”ç¤ºå„ç§æ­£åˆ™åŒ–æŠ€æœ¯çš„æ•ˆæœ
        åŒ…æ‹¬L1/L2æ­£åˆ™åŒ–ã€Dropoutã€æ—©åœç­‰
        """
        # åˆ›å»ºå®¹æ˜“è¿‡æ‹Ÿåˆçš„å°æ•°æ®é›†
        torch.manual_seed(123)
        n_samples = 200
        n_features = 50
        
        X = torch.randn(n_samples, n_features)
        # åªæœ‰å‰10ä¸ªç‰¹å¾æœ‰ç”¨ï¼Œå…¶ä½™ä¸ºå™ªå£°
        true_w = torch.zeros(n_features, 1)
        true_w[:10] = torch.randn(10, 1)
        y = X @ true_w + 0.1 * torch.randn(n_samples, 1)
        
        # åˆ’åˆ†æ•°æ®
        train_size = 100
        X_train, X_val = X[:train_size], X[train_size:]
        y_train, y_val = y[:train_size], y[train_size:]
        
        # ä¸åŒæ­£åˆ™åŒ–æ–¹æ³•
        configs = {
            'No_Reg': {'weight_decay': 0, 'dropout': 0},
            'L2_Reg': {'weight_decay': 0.01, 'dropout': 0},
            'Dropout': {'weight_decay': 0, 'dropout': 0.5},
            'L2_Dropout': {'weight_decay': 0.01, 'dropout': 0.3}
        }
        
        results = {}
        
        for config_name, config in configs.items():
            print(f"\næ­£åˆ™åŒ–é…ç½®: {config_name}")
            
            # åˆ›å»ºæ¨¡å‹
            model = nn.Sequential(
                nn.Linear(n_features, 100),
                nn.ReLU(),
                nn.Dropout(config['dropout']),
                nn.Linear(100, 50),
                nn.ReLU(),
                nn.Dropout(config['dropout']),
                nn.Linear(50, 1)
            )
            
            criterion = nn.MSELoss()
            optimizer = torch.optim.Adam(model.parameters(), 
                                       lr=0.001, 
                                       weight_decay=config['weight_decay'])
            
            # è®­ç»ƒå’ŒéªŒè¯æŸå¤±è®°å½•
            train_losses = []
            val_losses = []
            
            for epoch in range(500):
                # è®­ç»ƒ
                model.train()
                optimizer.zero_grad()
                train_pred = model(X_train)
                train_loss = criterion(train_pred, y_train)
                train_loss.backward()
                optimizer.step()
                
                # éªŒè¯
                model.eval()
                with torch.no_grad():
                    val_pred = model(X_val)
                    val_loss = criterion(val_pred, y_val)
                
                train_losses.append(train_loss.item())
                val_losses.append(val_loss.item())
            
            # åˆ†æè¿‡æ‹Ÿåˆç¨‹åº¦
            overfitting_degree = val_losses[-1] / train_losses[-1]
            
            results[config_name] = {
                'train_loss': train_losses[-1],
                'val_loss': val_losses[-1],
                'overfitting_degree': overfitting_degree
            }
            
            print(f"æœ€ç»ˆè®­ç»ƒæŸå¤±: {train_losses[-1]:.6f}")
            print(f"æœ€ç»ˆéªŒè¯æŸå¤±: {val_losses[-1]:.6f}")
            print(f"è¿‡æ‹Ÿåˆç¨‹åº¦: {overfitting_degree:.3f}")
        
        return results
    
    reg_results = regularization_comprehensive_demo()
    
    # 6. è´¹æ›¼å­¦ä¹ æ³•ï¼šé€šè¿‡å¯è§†åŒ–ç†è§£æ·±åº¦å­¦ä¹ 
    print("\n========== è´¹æ›¼å­¦ä¹ æ³•ï¼šå¯è§†åŒ–ç†è§£æ·±åº¦å­¦ä¹  ==========")
    
    def visualize_neural_network_learning():
        """
        å¯è§†åŒ–ç¥ç»ç½‘ç»œçš„å­¦ä¹ è¿‡ç¨‹
        é€šè¿‡ç®€å•çš„2Dåˆ†ç±»é—®é¢˜å±•ç¤ºå†³ç­–è¾¹ç•Œçš„å˜åŒ–
        """
        # ç”Ÿæˆèºæ—‹æ•°æ®
        def make_spiral_data(n_points=100):
            np.random.seed(0)
            N = n_points  # æ¯ç±»ç‚¹æ•°
            D = 2  # ç»´åº¦
            K = 3  # ç±»åˆ«æ•°
            X = np.zeros((N*K, D))
            y = np.zeros(N*K, dtype='uint8')
            
            for j in range(K):
                ix = range(N*j, N*(j+1))
                r = np.linspace(0.0, 1, N)  # åŠå¾„
                t = np.linspace(j*4, (j+1)*4, N) + np.random.randn(N)*0.2  # è§’åº¦
                X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]
                y[ix] = j
            
            return torch.FloatTensor(X), torch.LongTensor(y)
        
        X, y = make_spiral_data(200)
        
        # åˆ›å»ºç®€å•çš„ç¥ç»ç½‘ç»œ
        class SpiralClassifier(nn.Module):
            def __init__(self, hidden_size=50):
                super().__init__()
                self.net = nn.Sequential(
                    nn.Linear(2, hidden_size),
                    nn.ReLU(),
                    nn.Linear(hidden_size, hidden_size),
                    nn.ReLU(),
                    nn.Linear(hidden_size, 3)
                )
            
            def forward(self, x):
                return self.net(x)
        
        model = SpiralClassifier(100)
        criterion = nn.CrossEntropyLoss()
        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
        
        # è®­ç»ƒè¿‡ç¨‹è®°å½•
        epochs = [0, 50, 100, 200, 500]
        training_snapshots = {}
        
        print("è®­ç»ƒèºæ—‹åˆ†ç±»å™¨ï¼Œè®°å½•å†³ç­–è¾¹ç•Œå˜åŒ–ï¼š")
        
        for epoch in range(501):
            optimizer.zero_grad()
            outputs = model(X)
            loss = criterion(outputs, y)
            loss.backward()
            optimizer.step()
            
            if epoch in epochs:
                model.eval()
                with torch.no_grad():
                    train_acc = (outputs.argmax(dim=1) == y).float().mean().item()
                    training_snapshots[epoch] = {
                        'loss': loss.item(),
                        'accuracy': train_acc,
                        'model_state': model.state_dict().copy()
                    }
                    print(f"Epoch {epoch}: Loss={loss.item():.4f}, Acc={train_acc:.3f}")
                model.train()
        
        return model, X, y, training_snapshots
    
    spiral_model, spiral_X, spiral_y, snapshots = visualize_neural_network_learning()
    
    # 7. ç°ä»£è®­ç»ƒæŠ€å·§ç»¼åˆåº”ç”¨
    print("\n========== ç°ä»£è®­ç»ƒæŠ€å·§ï¼šå·¥ç¨‹æœ€ä½³å®è·µ ==========")
    
    def modern_training_pipeline():
        """
        ç°ä»£æ·±åº¦å­¦ä¹ è®­ç»ƒç®¡é“
        åŒ…å«æ•°æ®åŠ è½½ã€æ¨¡å‹å®šä¹‰ã€è®­ç»ƒå¾ªç¯ã€éªŒè¯ã€ä¿å­˜ç­‰å®Œæ•´æµç¨‹
        """
        # 1. æ•°æ®å‡†å¤‡
        print("1. æ•°æ®å‡†å¤‡ä¸é¢„å¤„ç†")
        
        # ç”Ÿæˆå¤§è§„æ¨¡æ•°æ®é›†
        n_samples = 10000
        n_features = 100
        n_classes = 10
        
        # ç‰¹å¾æ ‡å‡†åŒ–çš„é‡è¦æ€§
        X_raw = torch.randn(n_samples, n_features) * 10 + 5
        y = torch.randint(0, n_classes, (n_samples,))
        
        # æ ‡å‡†åŒ–
        X_mean = X_raw.mean(dim=0)
        X_std = X_raw.std(dim=0)
        X = (X_raw - X_mean) / (X_std + 1e-8)
        
        print(f"åŸå§‹æ•°æ®èŒƒå›´: [{X_raw.min().item():.2f}, {X_raw.max().item():.2f}]")
        print(f"æ ‡å‡†åŒ–åèŒƒå›´: [{X.min().item():.2f}, {X.max().item():.2f}]")
        
        # æ•°æ®é›†åˆ’åˆ†
        indices = torch.randperm(n_samples)
        train_idx = indices[:8000]
        val_idx = indices[8000:9000]
        test_idx = indices[9000:]
        
        train_dataset = TensorDataset(X[train_idx], y[train_idx])
        val_dataset = TensorDataset(X[val_idx], y[val_idx])
        test_dataset = TensorDataset(X[test_idx], y[test_idx])
        
        # æ•°æ®åŠ è½½å™¨
        train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=256, shuffle=False)
        test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False)
        
        # 2. æ¨¡å‹å®šä¹‰
        print("\n2. ç°ä»£ç¥ç»ç½‘ç»œæ¶æ„è®¾è®¡")
        
        class ModernMLP(nn.Module):
            """ç°ä»£MLPè®¾è®¡ï¼šæ‰¹å½’ä¸€åŒ–+æ®‹å·®è¿æ¥+è‡ªé€‚åº”æ¿€æ´»"""
            def __init__(self, input_dim, hidden_dims, output_dim):
                super().__init__()
                
                self.input_layer = nn.Sequential(
                    nn.Linear(input_dim, hidden_dims[0]),
                    nn.BatchNorm1d(hidden_dims[0]),
                    nn.ReLU()
                )
                
                self.hidden_layers = nn.ModuleList()
                for i in range(len(hidden_dims) - 1):
                    layer = nn.Sequential(
                        nn.Linear(hidden_dims[i], hidden_dims[i+1]),
                        nn.BatchNorm1d(hidden_dims[i+1]),
                        nn.ReLU(),
                        nn.Dropout(0.1)
                    )
                    self.hidden_layers.append(layer)
                
                self.output_layer = nn.Linear(hidden_dims[-1], output_dim)
                
            def forward(self, x):
                x = self.input_layer(x)
                
                for layer in self.hidden_layers:
                    residual = x
                    x = layer(x)
                    # ç®€å•æ®‹å·®è¿æ¥ï¼ˆç»´åº¦åŒ¹é…æ—¶ï¼‰
                    if x.shape == residual.shape:
                        x = x + residual
                
                x = self.output_layer(x)
                return x
        
        model = ModernMLP(n_features, [256, 128, 64], n_classes)
        
        # 3. ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
        print("\n3. ä¼˜åŒ–ç­–ç•¥é…ç½®")
        
        optimizer = torch.optim.AdamW(model.parameters(), 
                                    lr=0.001, 
                                    weight_decay=0.01)
        
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            optimizer, T_max=100, eta_min=1e-6
        )
        
        criterion = nn.CrossEntropyLoss(label_smoothing=0.1)
        
        # 4. è®­ç»ƒå¾ªç¯
        print("\n4. è®­ç»ƒè¿‡ç¨‹ç›‘æ§")
        
        def train_epoch(model, train_loader, optimizer, criterion):
            model.train()
            total_loss = 0
            correct = 0
            total = 0
            
            for batch_X, batch_y in train_loader:
                optimizer.zero_grad()
                outputs = model(batch_X)
                loss = criterion(outputs, batch_y)
                loss.backward()
                
                # æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                
                optimizer.step()
                
                total_loss += loss.item()
                predicted = outputs.argmax(dim=1)
                total += batch_y.size(0)
                correct += (predicted == batch_y).sum().item()
            
            return total_loss / len(train_loader), correct / total
        
        def validate_epoch(model, val_loader, criterion):
            model.eval()
            total_loss = 0
            correct = 0
            total = 0
            
            with torch.no_grad():
                for batch_X, batch_y in val_loader:
                    outputs = model(batch_X)
                    loss = criterion(outputs, batch_y)
                    
                    total_loss += loss.item()
                    predicted = outputs.argmax(dim=1)
                    total += batch_y.size(0)
                    correct += (predicted == batch_y).sum().item()
            
            return total_loss / len(val_loader), correct / total
        
        # è®­ç»ƒå†å²è®°å½•
        history = {
            'train_loss': [], 'train_acc': [],
            'val_loss': [], 'val_acc': [],
            'lr': []
        }
        
        # æ—©åœæœºåˆ¶
        best_val_loss = float('inf')
        patience = 10
        patience_counter = 0
        
        print("å¼€å§‹è®­ç»ƒ...")
        start_time = time.time()
        
        for epoch in range(100):
            # è®­ç»ƒ
            train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion)
            
            # éªŒè¯
            val_loss, val_acc = validate_epoch(model, val_loader, criterion)
            
            # å­¦ä¹ ç‡è°ƒåº¦
            scheduler.step()
            current_lr = optimizer.param_groups[0]['lr']
            
            # è®°å½•å†å²
            history['train_loss'].append(train_loss)
            history['train_acc'].append(train_acc)
            history['val_loss'].append(val_loss)
            history['val_acc'].append(val_acc)
            history['lr'].append(current_lr)
            
            # æ—©åœæ£€æŸ¥
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                patience_counter = 0
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                torch.save(model.state_dict(), 'best_model.pth')
            else:
                patience_counter += 1
            
            if epoch % 10 == 0:
                print(f"Epoch {epoch}: Train Loss={train_loss:.4f}, Train Acc={train_acc:.3f}, "
                      f"Val Loss={val_loss:.4f}, Val Acc={val_acc:.3f}, LR={current_lr:.6f}")
            
            # æ—©åœ
            if patience_counter >= patience:
                print(f"æ—©åœè§¦å‘ï¼Œåœ¨ç¬¬ {epoch} è½®åœæ­¢è®­ç»ƒ")
                break
        
        training_time = time.time() - start_time
        print(f"è®­ç»ƒå®Œæˆï¼Œæ€»ç”¨æ—¶: {training_time:.2f}ç§’")
        
        # 5. æœ€ç»ˆæµ‹è¯•
        print("\n5. æœ€ç»ˆæ¨¡å‹è¯„ä¼°")
        
        # åŠ è½½æœ€ä½³æ¨¡å‹
        model.load_state_dict(torch.load('best_model.pth'))
        test_loss, test_acc = validate_epoch(model, test_loader, criterion)
        
        print(f"æµ‹è¯•é›†æ€§èƒ½: Loss={test_loss:.4f}, Accuracy={test_acc:.3f}")
        
        return model, history
    
    final_model, training_history = modern_training_pipeline()
    
    # 8. æ¨¡å‹åˆ†æä¸è¯Šæ–­
    print("\n========== æ¨¡å‹åˆ†æä¸è¯Šæ–­å·¥å…· ==========")
    
    def model_diagnostic_tools(model, history):
        """
        æ·±åº¦å­¦ä¹ æ¨¡å‹è¯Šæ–­å·¥å…·
        åˆ†æè®­ç»ƒè¿‡ç¨‹ã€å‚æ•°åˆ†å¸ƒã€æ¢¯åº¦æµåŠ¨ç­‰
        """
        print("1. è®­ç»ƒæ›²çº¿åˆ†æ")
        final_train_loss = history['train_loss'][-1]
        final_val_loss = history['val_loss'][-1]
        final_train_acc = history['train_acc'][-1]
        final_val_acc = history['val_acc'][-1]
        
        print(f"æœ€ç»ˆè®­ç»ƒæŸå¤±: {final_train_loss:.4f}")
        print(f"æœ€ç»ˆéªŒè¯æŸå¤±: {final_val_loss:.4f}")
        print(f"è¿‡æ‹Ÿåˆç¨‹åº¦: {final_val_loss/final_train_loss:.3f}")
        print(f"å‡†ç¡®ç‡å·®è·: {final_train_acc - final_val_acc:.3f}")
        
        # åˆ¤æ–­è®­ç»ƒçŠ¶æ€
        if final_val_loss/final_train_loss > 1.5:
            print("è¯Šæ–­: å­˜åœ¨è¿‡æ‹Ÿåˆï¼Œå»ºè®®å¢åŠ æ­£åˆ™åŒ–")
        elif final_train_acc < 0.7:
            print("è¯Šæ–­: å­˜åœ¨æ¬ æ‹Ÿåˆï¼Œå»ºè®®å¢åŠ æ¨¡å‹å¤æ‚åº¦")
        else:
            print("è¯Šæ–­: æ¨¡å‹è®­ç»ƒçŠ¶æ€è‰¯å¥½")
        
        print("\n2. å‚æ•°åˆ†å¸ƒåˆ†æ")
        for name, param in model.named_parameters():
            if param.requires_grad and 'weight' in name:
                weight_std = param.data.std().item()
                weight_mean = param.data.mean().item()
                print(f"{name}: å‡å€¼={weight_mean:.6f}, æ ‡å‡†å·®={weight_std:.6f}")
        
        print("\n3. æ¨¡å‹å¤æ‚åº¦ç»Ÿè®¡")
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        
        print(f"æ€»å‚æ•°é‡: {total_params:,}")
        print(f"å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
        
        # è®¡ç®—æ¨¡å‹å¤§å°
        model_size = sum(p.numel() * p.element_size() for p in model.parameters()) / (1024**2)
        print(f"æ¨¡å‹å¤§å°: {model_size:.2f} MB")
        
    model_diagnostic_tools(final_model, training_history)
    
    # 9. æ·±åº¦å­¦ä¹ è°ƒè¯•æŠ€å·§
    print("\n========== æ·±åº¦å­¦ä¹ è°ƒè¯•æŠ€å·§ ==========")
    
    def debugging_techniques():
        """
        æ·±åº¦å­¦ä¹ å¸¸è§é—®é¢˜çš„è°ƒè¯•æ–¹æ³•
        æä¾›ç³»ç»Ÿæ€§çš„é—®é¢˜è¯Šæ–­å’Œè§£å†³æ–¹æ¡ˆ
        """
        print("å¸¸è§é—®é¢˜è¯Šæ–­æ¸…å•ï¼š")
        
        debugging_checklist = {
            "æŸå¤±ä¸ä¸‹é™": [
                "æ£€æŸ¥å­¦ä¹ ç‡æ˜¯å¦è¿‡å¤§æˆ–è¿‡å°",
                "éªŒè¯æ•°æ®é¢„å¤„ç†æ˜¯å¦æ­£ç¡®",
                "ç¡®è®¤æ ‡ç­¾æ ¼å¼ä¸æŸå¤±å‡½æ•°åŒ¹é…",
                "æ£€æŸ¥æ¨¡å‹æ¶æ„åˆç†æ€§"
            ],
            "è¿‡æ‹Ÿåˆä¸¥é‡": [
                "å¢åŠ æ­£åˆ™åŒ–ï¼ˆL2ã€Dropoutï¼‰",
                "å‡å°‘æ¨¡å‹å¤æ‚åº¦",
                "å¢åŠ è®­ç»ƒæ•°æ®",
                "ä½¿ç”¨æ•°æ®å¢å¼º"
            ],
            "è®­ç»ƒé€Ÿåº¦æ…¢": [
                "å¢å¤§æ‰¹é‡å¤§å°",
                "ä½¿ç”¨æ›´é«˜æ•ˆçš„ä¼˜åŒ–å™¨",
                "æ£€æŸ¥æ•°æ®åŠ è½½ç“¶é¢ˆ",
                "è€ƒè™‘æ··åˆç²¾åº¦è®­ç»ƒ"
            ],
            "æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸": [
                "ä½¿ç”¨æ‰¹å½’ä¸€åŒ–",
                "è°ƒæ•´åˆå§‹åŒ–æ–¹æ³•",
                "ä½¿ç”¨æ¢¯åº¦è£å‰ª",
                "è€ƒè™‘æ®‹å·®è¿æ¥"
            ]
        }
        
        for problem, solutions in debugging_checklist.items():
            print(f"\n{problem}:")
            for i, solution in enumerate(solutions, 1):
                print(f"  {i}. {solution}")
        
        # ç®€å•çš„æŸå¤±å‡½æ•°éªŒè¯
        print("\nå®ç”¨è°ƒè¯•å·¥å…·ç¤ºä¾‹ï¼š")
        
        def sanity_check_model(model, sample_input, expected_output_shape):
            """æ¨¡å‹åŸºæœ¬åŠŸèƒ½æ£€æŸ¥"""
            try:
                model.eval()
                with torch.no_grad():
                    output = model(sample_input)
                    
                print(f"âœ“ å‰å‘ä¼ æ’­æˆåŠŸ")
                print(f"âœ“ è¾“å‡ºå½¢çŠ¶: {output.shape} (æœŸæœ›: {expected_output_shape})")
                
                if output.shape == expected_output_shape:
                    print("âœ“ è¾“å‡ºå½¢çŠ¶åŒ¹é…")
                else:
                    print("âœ— è¾“å‡ºå½¢çŠ¶ä¸åŒ¹é…")
                
                # æ£€æŸ¥æ•°å€¼ç¨³å®šæ€§
                if torch.isnan(output).any():
                    print("âœ— è¾“å‡ºåŒ…å«NaN")
                elif torch.isinf(output).any():
                    print("âœ— è¾“å‡ºåŒ…å«Inf")
                else:
                    print("âœ“ æ•°å€¼ç¨³å®š")
                    
            except Exception as e:
                print(f"âœ— å‰å‘ä¼ æ’­å¤±è´¥: {e}")
        
        # æµ‹è¯•å½“å‰æ¨¡å‹
        sample_input = torch.randn(1, 100)
        sanity_check_model(final_model, sample_input, (1, 10))
    
    debugging_techniques()
    
    # 10. æ€»ç»“ä¸å±•æœ›
    print("\n========== æ·±åº¦å­¦ä¹ åŸºç¡€æ€»ç»“ä¸å±•æœ› ==========")
    print("""
    ã€æ ¸å¿ƒæŠ€æœ¯è¦ç‚¹ã€‘
    1. æ¢¯åº¦ä¸‹é™ï¼šæ·±åº¦å­¦ä¹ ä¼˜åŒ–çš„åŸºç¡€ï¼Œç†è§£å‚æ•°æ›´æ–°æœºåˆ¶
    2. éçº¿æ€§æ¿€æ´»ï¼šä½¿ç½‘ç»œå…·å¤‡å¤æ‚å‡½æ•°é€¼è¿‘èƒ½åŠ›
    3. æ­£åˆ™åŒ–æŠ€æœ¯ï¼šé˜²æ­¢è¿‡æ‹Ÿåˆï¼Œæå‡æ¨¡å‹æ³›åŒ–èƒ½åŠ›
    4. æ‰¹é‡å¤„ç†ï¼šæå‡è®­ç»ƒæ•ˆç‡å’Œç¨³å®šæ€§
    5. ç°ä»£è®­ç»ƒæŠ€å·§ï¼šæ‰¹å½’ä¸€åŒ–ã€æ®‹å·®è¿æ¥ã€å­¦ä¹ ç‡è°ƒåº¦
    
    ã€ç†è®ºåˆ°å®è·µçš„æ¡¥æ¢ã€‘
    - æ•°å­¦åŸç†ï¼šç†è§£æ¢¯åº¦ã€åå‘ä¼ æ’­ã€æŸå¤±å‡½æ•°çš„æ•°å­¦åŸºç¡€
    - å·¥ç¨‹å®ç°ï¼šæŒæ¡PyTorchçš„æ¨¡å—åŒ–è®¾è®¡å’Œæœ€ä½³å®è·µ
    - è°ƒè¯•æŠ€èƒ½ï¼šç³»ç»Ÿæ€§çš„é—®é¢˜è¯Šæ–­å’Œè§£å†³èƒ½åŠ›
    
    ã€æœªæ¥å­¦ä¹ è·¯å¾„ã€‘
    1. ä¸“ä¸šé¢†åŸŸï¼šCNN(è®¡ç®—æœºè§†è§‰)ã€RNN(åºåˆ—å»ºæ¨¡)ã€Transformer(æ³¨æ„åŠ›æœºåˆ¶)
    2. é«˜çº§æŠ€æœ¯ï¼šç”Ÿæˆæ¨¡å‹ã€å¼ºåŒ–å­¦ä¹ ã€å…ƒå­¦ä¹ 
    3. å·¥ç¨‹ä¼˜åŒ–ï¼šæ¨¡å‹å‹ç¼©ã€é‡åŒ–ã€åˆ†å¸ƒå¼è®­ç»ƒ
    4. éƒ¨ç½²åº”ç”¨ï¼šè¾¹ç¼˜è®¡ç®—ã€å®æ—¶æ¨ç†ã€A/Bæµ‹è¯•
    """)
    
    print("\nğŸ¯ æ·±åº¦å­¦ä¹ åŸºç¡€æŒæ¡å»ºè®®ï¼š")
    print("1. ç†è®ºåŸºç¡€ï¼šæŒæ¡çº¿æ€§ä»£æ•°ã€æ¦‚ç‡è®ºã€ä¼˜åŒ–ç†è®º")
    print("2. ç¼–ç¨‹å®è·µï¼šç†Ÿç»ƒä½¿ç”¨PyTorchè¿›è¡Œæ¨¡å‹å®ç°")
    print("3. é—®é¢˜è§£å†³ï¼šåŸ¹å…»ç³»ç»Ÿæ€§çš„è°ƒè¯•å’Œä¼˜åŒ–èƒ½åŠ›")
    print("4. æŒç»­å­¦ä¹ ï¼šè·Ÿè¸ªæœ€æ–°ç ”ç©¶è¿›å±•å’ŒæŠ€æœ¯å‘å±•")
    print("5. é¡¹ç›®ç»éªŒï¼šåœ¨å®é™…é—®é¢˜ä¸­åº”ç”¨å’ŒéªŒè¯æ‰€å­¦çŸ¥è¯†")

# è¿è¡Œè®­ç»ƒåæ¨¡å‹é¢„æµ‹ç¤ºä¾‹
# åŸç†è¯´æ˜ï¼š
# è®­ç»ƒå®Œæˆåï¼Œæ¨¡å‹å¯ä»¥å¯¹æ–°æ ·æœ¬è¿›è¡Œé¢„æµ‹ã€‚é€šè¿‡softmaxè¾“å‡ºæœ€å¤§æ¦‚ç‡çš„ç±»åˆ«ä½œä¸ºæœ€ç»ˆé¢„æµ‹ç»“æœã€‚ 