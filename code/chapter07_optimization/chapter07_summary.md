# PyTorch 优化算法模块学习笔记

---

## 7.1_optimization-intro.ipynb

### 1. 功能说明
介绍优化算法的基本概念，包括目标函数、参数空间、梯度下降思想。

### 2. 核心逻辑与原理细节
- **目标函数**：定义优化目标，通常为损失函数。
- **参数空间**：优化变量的集合。
- **梯度下降**：通过梯度信息更新参数，逐步逼近最优解。

### 3. 应用场景
- 任何需要最小化损失函数的深度学习任务。

### 4. 调用关系
- 独立介绍优化基础，为后续算法做铺垫。

---

## 7.2_gd-sgd.ipynb

### 1. 功能说明
讲解梯度下降（GD）与随机梯度下降（SGD）的原理与实现。

### 2. 核心逻辑与原理细节
- **批量梯度下降**：每次用全部样本计算梯度，收敛稳定但慢。
- **随机梯度下降**：每次用单一样本，更新快但波动大。
- **对比分析**：GD适合小数据，SGD适合大数据。

### 3. 应用场景
- 训练神经网络、线性回归等。

### 4. 调用关系
- 独立实现，常作为优化器基础。

---

## 7.3_minibatch-sgd.ipynb

### 1. 功能说明
介绍小批量随机梯度下降（Mini-batch SGD）及其优势。

### 2. 核心逻辑与原理细节
- **小批量采样**：每次用一小部分样本，兼顾效率与稳定性。
- **并行计算**：适合GPU加速。

### 3. 应用场景
- 现代深度学习训练的主流方法。

### 4. 调用关系
- 独立实现，常与其他优化器结合。

---

## 7.4_momentum.ipynb

### 1. 功能说明
讲解动量法（Momentum）优化器的原理与实现。

### 2. 核心逻辑与原理细节
- **动量机制**：引入历史梯度累积，抑制震荡，加速收敛。
- **对比分析**：比SGD更快收敛，适合鞍点和高曲率场景。

### 3. 应用场景
- 深度网络训练，尤其是收敛慢或震荡大的任务。

### 4. 调用关系
- 可与SGD结合，作为PyTorch优化器参数。

---

## 7.5_adagrad.ipynb

### 1. 功能说明
介绍Adagrad自适应学习率优化器。

### 2. 核心逻辑与原理细节
- **自适应学习率**：对每个参数分配独立学习率，适合稀疏特征。
- **对比分析**：适合NLP等稀疏场景，但学习率单调递减。

### 3. 应用场景
- 词嵌入、NLP、稀疏数据。

### 4. 调用关系
- 可直接作为PyTorch优化器。

---

## 7.6_rmsprop.ipynb

### 1. 功能说明
介绍RMSProp优化器，解决Adagrad学习率过快下降问题。

### 2. 核心逻辑与原理细节
- **指数加权平均**：平滑历史梯度平方，保持有效学习率。
- **对比分析**：适合非平稳目标，广泛用于RNN等。

### 3. 应用场景
- RNN、深度网络训练。

### 4. 调用关系
- 可直接作为PyTorch优化器。

---

## 7.7_adadelta.ipynb

### 1. 功能说明
介绍Adadelta优化器，进一步改进自适应学习率。

### 2. 核心逻辑与原理细节
- **无须手动设定全局学习率**，自适应调整。
- **对比分析**：与RMSProp类似，适合深度网络。

### 3. 应用场景
- 深度学习各类任务。

### 4. 调用关系
- 可直接作为PyTorch优化器。

---

## 7.8_adam.ipynb

### 1. 功能说明
介绍Adam优化器，结合动量和自适应学习率。

### 2. 核心逻辑与原理细节
- **一阶矩与二阶矩估计**：融合Momentum和RMSProp优点。
- **对比分析**：收敛快，鲁棒性强，是深度学习默认优化器。

### 3. 应用场景
- 几乎所有深度学习任务，尤其是大规模模型。

### 4. 调用关系
- 可直接作为PyTorch优化器。

---

## 高层次总结
本章系统梳理了深度学习常用优化算法，从基础的梯度下降到自适应方法，逐步引入动量、学习率调整等机制。各优化器适用于不同场景，实际应用中需结合任务特点选择。理解其原理有助于调参和模型收敛。 