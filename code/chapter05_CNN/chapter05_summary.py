"""
PyTorch å·ç§¯ç¥ç»ç½‘ç»œæ ¸å¿ƒæŠ€æœ¯ä¸æ·±åº¦ç†è§£
-----------------------------------
ã€æ–‡ä»¶è¯´æ˜ã€‘
æœ¬æ–‡ä»¶ç³»ç»Ÿæ¢³ç†äº†å·ç§¯ç¥ç»ç½‘ç»œçš„æ ¸å¿ƒæŠ€æœ¯ï¼ŒåŒ…æ‹¬ï¼š
- å·ç§¯å±‚ä¸æ± åŒ–å±‚ï¼šç©ºé—´ç‰¹å¾æå–çš„åŸºç¡€
- ç»å…¸CNNæ¶æ„ï¼šLeNetã€AlexNetã€VGGã€ResNetã€DenseNetç­‰
- ç½‘ç»œè®¾è®¡åŸç†ï¼šæ·±åº¦ã€å®½åº¦ã€è·³è·ƒè¿æ¥çš„ä½œç”¨
- ç°ä»£CNNæŠ€æœ¯ï¼šæ‰¹å½’ä¸€åŒ–ã€æ®‹å·®è¿æ¥ã€æ³¨æ„åŠ›æœºåˆ¶
- å®é™…åº”ç”¨ä¸å·¥ç¨‹å®è·µ

ã€ç¬¬ä¸€æ€§åŸç†æ€è€ƒã€‘
1. ä¸ºä»€ä¹ˆéœ€è¦å·ç§¯ç¥ç»ç½‘ç»œï¼Ÿ
   - å›¾åƒå…·æœ‰ç©ºé—´å±€éƒ¨æ€§å’Œå¹³ç§»ä¸å˜æ€§
   - å…¨è¿æ¥å±‚å‚æ•°è¿‡å¤šï¼Œå®¹æ˜“è¿‡æ‹Ÿåˆ
   - å·ç§¯å±‚é€šè¿‡æƒé‡å…±äº«å¤§å¹…å‡å°‘å‚æ•°

2. æ·±å±‚ç½‘ç»œä¸ºä»€ä¹ˆæ›´æœ‰æ•ˆï¼Ÿ
   - å±‚æ¬¡ç‰¹å¾è¡¨ç¤ºï¼šä½å±‚â†’è¾¹ç¼˜ï¼Œé«˜å±‚â†’è¯­ä¹‰
   - æ›´å¤§çš„æ„Ÿå—é‡èƒ½æ•è·æ›´å¤æ‚çš„æ¨¡å¼
   - éçº¿æ€§å˜æ¢çš„å¤åˆå¢å¼ºè¡¨è¾¾èƒ½åŠ›

3. æ®‹å·®è¿æ¥è§£å†³ä»€ä¹ˆé—®é¢˜ï¼Ÿ
   - æ¢¯åº¦æ¶ˆå¤±å¯¼è‡´æ·±å±‚ç½‘ç»œéš¾ä»¥è®­ç»ƒ
   - æ’ç­‰æ˜ å°„ä¿è¯ä¿¡æ¯ä¼ æ’­
   - ä½¿ç½‘ç»œèƒ½å¤Ÿè®­ç»ƒå¾—æ›´æ·±

ã€è‹æ ¼æ‹‰åº•å¼æé—®ä¸éªŒè¯ã€‘
1. æ›´æ·±çš„ç½‘ç»œæ€»æ˜¯æ›´å¥½å—ï¼Ÿ
   - é—®é¢˜ï¼šæ·±åº¦ä¸æ€§èƒ½çš„å…³ç³»æ˜¯ä»€ä¹ˆï¼Ÿ
   - éªŒè¯ï¼šé€šè¿‡ä¸åŒæ·±åº¦çš„ç½‘ç»œå¯¹æ¯”
   - ç»“è®ºï¼šéœ€è¦åˆé€‚çš„è®¾è®¡é¿å…é€€åŒ–é—®é¢˜

2. å·ç§¯æ ¸å¤§å°å¦‚ä½•é€‰æ‹©ï¼Ÿ
   - é—®é¢˜ï¼šå¤§æ ¸è¿˜æ˜¯å°æ ¸æ›´å¥½ï¼Ÿ
   - éªŒè¯ï¼šé€šè¿‡æ„Ÿå—é‡å’Œè®¡ç®—æ•ˆç‡åˆ†æ
   - ç»“è®ºï¼šå¤šä¸ªå°æ ¸é€šå¸¸ä¼˜äºå•ä¸ªå¤§æ ¸

ã€è´¹æ›¼å­¦ä¹ æ³•è®²è§£ã€‘
1. æ¦‚å¿µè§£é‡Š
   - ç”¨æ˜¾å¾®é•œç±»æ¯”å·ç§¯çš„å±€éƒ¨è§‚å¯Ÿ
   - ç”¨å»ºç­‘è®¾è®¡ç±»æ¯”ç½‘ç»œæ¶æ„
   - å¼ºè°ƒCNNåœ¨è®¡ç®—æœºè§†è§‰ä¸­çš„é‡è¦æ€§

2. å®ä¾‹æ•™å­¦
   - ä»ç®€å•çš„è¾¹ç¼˜æ£€æµ‹å¼€å§‹
   - é€æ­¥æ„å»ºå¤æ‚çš„åˆ†ç±»ç½‘ç»œ
   - é€šè¿‡å¯è§†åŒ–ç†è§£ç‰¹å¾å­¦ä¹ 

ã€è®¾è®¡æ„ä¹‰ä¸å·¥ç¨‹ä»·å€¼ã€‘
- CNNæ˜¯è®¡ç®—æœºè§†è§‰çš„åŸºçŸ³ï¼Œå½±å“äº†æ•´ä¸ªæ·±åº¦å­¦ä¹ é¢†åŸŸ
- ç°ä»£è§†è§‰æ¨¡å‹çš„æ ¸å¿ƒæ€æƒ³éƒ½æºäºCNNçš„è®¾è®¡åŸç†
- ç†è§£CNNå¯¹æŒæ¡è§†è§‰Transformerç­‰æ–°æ¶æ„ä¹Ÿå¾ˆé‡è¦

å¯è¿è¡Œæ¡ˆä¾‹ï¼š
"""

import torch
from torch import nn
import numpy as np

# 1. åŸºç¡€å·ç§¯å±‚
class SimpleConv(nn.Module):
    """
    åŸºç¡€äºŒç»´å·ç§¯ + æ¿€æ´» + æ± åŒ–
    åŸç†ï¼šå·ç§¯å±‚ç”¨äºæå–å±€éƒ¨ç©ºé—´ç‰¹å¾ï¼Œæ± åŒ–å±‚é™ä½ç‰¹å¾å›¾å°ºå¯¸ï¼Œæå‡ä¸å˜æ€§ã€‚
    """
    def __init__(self):
        super().__init__()
        self.conv = nn.Conv2d(1, 6, kernel_size=5)  # 5x5å·ç§¯æ ¸ï¼Œ1è¾“å…¥é€šé“ï¼Œ6è¾“å‡ºé€šé“
        self.pool = nn.MaxPool2d(2, 2)              # 2x2æœ€å¤§æ± åŒ–
    def forward(self, x):
        # å…ˆå·ç§¯æç‰¹å¾ï¼Œå†æ¿€æ´»ï¼Œå†æ± åŒ–é™é‡‡æ ·
        return self.pool(torch.relu(self.conv(x)))

if __name__ == "__main__":
    # 1. åŸºç¡€å·ç§¯å±‚è°ƒç”¨æ¡ˆä¾‹
    x = torch.randn(1, 1, 28, 28)
    model = SimpleConv()
    out = model(x)
    print("SimpleConvè¾“å‡ºå½¢çŠ¶:", out.shape)  # [1, 6, 12, 12]

    # 2. å¤šé€šé“å·ç§¯
    conv2d = nn.Conv2d(in_channels=3, out_channels=8, kernel_size=3, padding=1, stride=1)
    # åŸç†ï¼šå¤šè¾“å…¥/è¾“å‡ºé€šé“ç”¨äºå¤„ç†å½©è‰²å›¾åƒå’Œä¸°å¯Œç‰¹å¾è¡¨è¾¾
    x = torch.randn(4, 3, 32, 32)
    out = conv2d(x)
    print("å¤šé€šé“å·ç§¯è¾“å‡ºå½¢çŠ¶:", out.shape)  # [4, 8, 32, 32]

    # 3. æ± åŒ–å±‚
    pool = nn.MaxPool2d(2, 2)
    # åŸç†ï¼šæ± åŒ–é™ä½ç‰¹å¾å›¾å°ºå¯¸ï¼Œå‡å°‘å‚æ•°ï¼Œæå‡å¹³ç§»ä¸å˜æ€§
    x = torch.randn(2, 8, 16, 16)
    out = pool(x)
    print("æ± åŒ–è¾“å‡ºå½¢çŠ¶:", out.shape)  # [2, 8, 8, 8]

    # 4. LeNet ç»“æ„
    class LeNet(nn.Module):
        """
        LeNet: ç»å…¸çš„å·ç§¯ç¥ç»ç½‘ç»œç»“æ„
        åŸç†ï¼šé€šè¿‡å·ç§¯-æ± åŒ–-å…¨è¿æ¥çš„å †å ï¼Œå®ç°ç«¯åˆ°ç«¯çš„å›¾åƒåˆ†ç±»
        é€‚ç”¨åœºæ™¯ï¼šå…¥é—¨çº§å›¾åƒåˆ†ç±»ä»»åŠ¡ï¼Œç»“æ„ç®€å•ï¼Œæ˜“äºç†è§£ã€‚
        """
        def __init__(self):
            super().__init__()
            self.conv = nn.Sequential(
                nn.Conv2d(1, 6, 5), nn.Sigmoid(), nn.MaxPool2d(2, 2),
                nn.Conv2d(6, 16, 5), nn.Sigmoid(), nn.MaxPool2d(2, 2)
            )
            self.fc = nn.Sequential(
                nn.Linear(16*4*4, 120), nn.Sigmoid(),
                nn.Linear(120, 84), nn.Sigmoid(),
                nn.Linear(84, 10)
            )
        def forward(self, x):
            x = self.conv(x)
            x = x.view(x.shape[0], -1)
            return self.fc(x)
    x = torch.randn(8, 1, 28, 28)
    net = LeNet()
    out = net(x)
    print("LeNetè¾“å‡ºå½¢çŠ¶:", out.shape)  # [8, 10]

    # 5. AlexNet ç»“æ„
    class AlexNet(nn.Module):
        """
        AlexNet: æ·±å±‚å¤§è§„æ¨¡å·ç§¯ç¥ç»ç½‘ç»œ
        åŸç†ï¼šæ›´æ·±çš„ç½‘ç»œç»“æ„ã€æ›´å¤§çš„å·ç§¯æ ¸å’Œé€šé“æ•°ï¼Œä½¿ç”¨ReLUå’ŒDropoutæå‡æ€§èƒ½ã€‚
        é€‚ç”¨åœºæ™¯ï¼šå¤§è§„æ¨¡å›¾åƒåˆ†ç±»ä»»åŠ¡ï¼Œç‰¹å¾è¡¨è¾¾èƒ½åŠ›å¼ºã€‚
        """
        def __init__(self):
            super().__init__()
            self.features = nn.Sequential(
                nn.Conv2d(1, 96, kernel_size=11, stride=4, padding=1), nn.ReLU(), nn.MaxPool2d(3, 2),
                nn.Conv2d(96, 256, kernel_size=5, padding=2), nn.ReLU(), nn.MaxPool2d(3, 2),
                nn.Conv2d(256, 384, kernel_size=3, padding=1), nn.ReLU(),
                nn.Conv2d(384, 384, kernel_size=3, padding=1), nn.ReLU(),
                nn.Conv2d(384, 256, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(3, 2)
            )
            self.classifier = nn.Sequential(
                nn.Dropout(0.5), nn.Linear(256*6*6, 4096), nn.ReLU(),
                nn.Dropout(0.5), nn.Linear(4096, 4096), nn.ReLU(),
                nn.Linear(4096, 10)
            )
        def forward(self, x):
            x = self.features(x)
            # Debug: print feature output shape
            # print(f"Features output shape: {x.shape}")
            x = x.view(x.shape[0], -1)
            return self.classifier(x)
    x = torch.randn(4, 1, 224, 224)
    alexnet = AlexNet()
    
    # First check the feature output dimension
    alexnet.eval()
    with torch.no_grad():
        features_out = alexnet.features(x)
        flattened_size = features_out.view(features_out.shape[0], -1).shape[1]
        print(f"AlexNetç‰¹å¾è¾“å‡ºå½¢çŠ¶: {features_out.shape}, å±•å¹³åå¤§å°: {flattened_size}")
    
    # Fix the classifier to match actual dimensions
    alexnet.classifier = nn.Sequential(
        nn.Dropout(0.5), nn.Linear(flattened_size, 4096), nn.ReLU(),
        nn.Dropout(0.5), nn.Linear(4096, 4096), nn.ReLU(),
        nn.Linear(4096, 10)
    )
    
    out = alexnet(x)
    print("AlexNetè¾“å‡ºå½¢çŠ¶:", out.shape)  # [4, 10]

    # 6. VGG ç»“æ„
    class VGGBlock(nn.Module):
        """
        VGG å—ï¼šå¤šä¸ªç›¸åŒå‚æ•°çš„å·ç§¯å±‚å †å ï¼Œæå‡ç‰¹å¾æå–èƒ½åŠ›ã€‚
        """
        def __init__(self, in_channels, out_channels, num_convs):
            super().__init__()
            layers = []
            for _ in range(num_convs):
                layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1))
                layers.append(nn.ReLU())
                in_channels = out_channels
            self.block = nn.Sequential(*layers)
        def forward(self, x):
            return self.block(x)
    class VGG(nn.Module):
        """
        VGG: é‡å¤å·ç§¯å—+å…¨è¿æ¥å±‚
        åŸç†ï¼šé€šè¿‡å †å å¤šä¸ªVGGBlockï¼Œæå‡ç½‘ç»œæ·±åº¦å’Œç‰¹å¾è¡¨è¾¾èƒ½åŠ›ã€‚
        é€‚ç”¨åœºæ™¯ï¼šæ·±å±‚ç½‘ç»œè®¾è®¡ï¼Œè¿ç§»å­¦ä¹ ã€‚
        """
        def __init__(self):
            super().__init__()
            self.features = nn.Sequential(
                VGGBlock(1, 64, 2), nn.MaxPool2d(2, 2),
                VGGBlock(64, 128, 2), nn.MaxPool2d(2, 2),
                VGGBlock(128, 256, 3), nn.MaxPool2d(2, 2),
                VGGBlock(256, 512, 3), nn.MaxPool2d(2, 2),
                VGGBlock(512, 512, 3), nn.MaxPool2d(2, 2)
            )
            self.classifier = nn.Sequential(
                nn.Linear(512, 4096), nn.ReLU(), nn.Dropout(0.5),
                nn.Linear(4096, 4096), nn.ReLU(), nn.Dropout(0.5),
                nn.Linear(4096, 10)
            )
        def forward(self, x):
            x = self.features(x)
            x = x.view(x.shape[0], -1)
            return self.classifier(x)
    x = torch.randn(2, 1, 32, 32)
    vgg = VGG()
    out = vgg(x)
    print("VGGè¾“å‡ºå½¢çŠ¶:", out.shape)  # [2, 10]

    # 7. GoogLeNet (Inception) ç»“æ„
    class Inception(nn.Module):
        """
        Inception å—ï¼šå¤šåˆ†æ”¯å¹¶è¡Œå·ç§¯ï¼Œèåˆå¤šå°ºåº¦ç‰¹å¾ã€‚
        """
        def __init__(self, in_channels, c1, c2, c3, c4):
            super().__init__()
            # 1x1å·ç§¯
            self.p1_1 = nn.Conv2d(in_channels, c1, kernel_size=1)
            # 1x1å·ç§¯åæ¥3x3å·ç§¯
            self.p2_1 = nn.Conv2d(in_channels, c2[0], kernel_size=1)
            self.p2_2 = nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1)
            # 1x1å·ç§¯åæ¥5x5å·ç§¯
            self.p3_1 = nn.Conv2d(in_channels, c3[0], kernel_size=1)
            self.p3_2 = nn.Conv2d(c3[0], c3[1], kernel_size=5, padding=2)
            # 3x3æœ€å¤§æ± åŒ–åæ¥1x1å·ç§¯
            self.p4_1 = nn.MaxPool2d(3, stride=1, padding=1)
            self.p4_2 = nn.Conv2d(in_channels, c4, kernel_size=1)
        def forward(self, x):
            p1 = torch.relu(self.p1_1(x))
            p2 = torch.relu(self.p2_2(torch.relu(self.p2_1(x))))
            p3 = torch.relu(self.p3_2(torch.relu(self.p3_1(x))))
            p4 = torch.relu(self.p4_2(self.p4_1(x)))
            return torch.cat([p1, p2, p3, p4], dim=1)
    class GoogLeNet(nn.Module):
        """
        GoogLeNet: å¤šåˆ†æ”¯Inceptionç»“æ„ï¼Œæå‡å¤šå°ºåº¦ç‰¹å¾èåˆèƒ½åŠ›ã€‚
        é€‚ç”¨åœºæ™¯ï¼šå¤æ‚å›¾åƒåˆ†ç±»ï¼Œå¤šå°ºåº¦ç‰¹å¾æå–ã€‚
        """
        def __init__(self):
            super().__init__()
            self.b1 = nn.Sequential(
                nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3), nn.ReLU(), nn.MaxPool2d(3, 2, padding=1)
            )
            self.b2 = nn.Sequential(
                nn.Conv2d(64, 64, kernel_size=1), nn.ReLU(),
                nn.Conv2d(64, 192, kernel_size=3, padding=1), nn.ReLU(), nn.MaxPool2d(3, 2, padding=1)
            )
            self.b3 = nn.Sequential(
                Inception(192, 64, (96, 128), (16, 32), 32),
                Inception(256, 128, (128, 192), (32, 96), 64), nn.MaxPool2d(3, 2, padding=1)
            )
            self.b4 = nn.Sequential(
                Inception(480, 192, (96, 208), (16, 48), 64),
                Inception(512, 160, (112, 224), (24, 64), 64),
                Inception(512, 128, (128, 256), (24, 64), 64),
                Inception(512, 112, (144, 288), (32, 64), 64),
                Inception(528, 256, (160, 320), (32, 128), 128), nn.MaxPool2d(3, 2, padding=1)
            )
            self.b5 = nn.Sequential(
                Inception(832, 256, (160, 320), (32, 128), 128),
                Inception(832, 384, (192, 384), (48, 128), 128),
                nn.AdaptiveAvgPool2d((1, 1)), nn.Flatten()
            )
            self.fc = nn.Linear(1024, 10)
        def forward(self, x):
            x = self.b1(x)
            x = self.b2(x)
            x = self.b3(x)
            x = self.b4(x)
            x = self.b5(x)
            return self.fc(x)
    x = torch.randn(2, 1, 96, 96)
    googlenet = GoogLeNet()
    out = googlenet(x)
    print("GoogLeNetè¾“å‡ºå½¢çŠ¶:", out.shape)  # [2, 10]

    # 8. DenseNet ç»“æ„
    class DenseBlock(nn.Module):
        """
        DenseBlock: æ‰€æœ‰å±‚ç‰¹å¾æ‹¼æ¥ï¼Œæå‡ç‰¹å¾å¤ç”¨å’Œæ¢¯åº¦æµåŠ¨ã€‚
        """
        def __init__(self, num_convs, in_channels, growth_rate):
            super().__init__()
            layers = []
            for i in range(num_convs):
                layers.append(self._conv_block(in_channels + i * growth_rate, growth_rate))
            self.net = nn.Sequential(*layers)
        def _conv_block(self, in_channels, out_channels):
            return nn.Sequential(
                nn.BatchNorm2d(in_channels), nn.ReLU(),
                nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)
            )
        def forward(self, x):
            for layer in self.net:
                y = layer(x)
                x = torch.cat([x, y], dim=1)
            return x
    class TransitionBlock(nn.Module):
        """
        è¿‡æ¸¡å±‚ï¼šæ§åˆ¶ç‰¹å¾å›¾å°ºå¯¸å’Œé€šé“æ•°ï¼Œé˜²æ­¢ç‰¹å¾çˆ†ç‚¸ã€‚
        """
        def __init__(self, in_channels, out_channels):
            super().__init__()
            self.block = nn.Sequential(
                nn.BatchNorm2d(in_channels), nn.ReLU(),
                nn.Conv2d(in_channels, out_channels, kernel_size=1),
                nn.AvgPool2d(2, 2)
            )
        def forward(self, x):
            return self.block(x)
    class DenseNet(nn.Module):
        """
        DenseNet: å¤šä¸ªDenseBlockå’Œè¿‡æ¸¡å±‚å †å ï¼Œæå‡ç‰¹å¾å¤ç”¨å’Œæ¢¯åº¦æµåŠ¨ã€‚
        é€‚ç”¨åœºæ™¯ï¼šé«˜æ•ˆæ·±å±‚ç½‘ç»œè®¾è®¡ã€‚
        """
        def __init__(self):
            super().__init__()
            self.net = nn.Sequential(
                nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(3, 2, padding=1),
                DenseBlock(2, 64, 32), TransitionBlock(128, 64),
                DenseBlock(2, 64, 32), TransitionBlock(128, 64),
                DenseBlock(2, 64, 32), nn.AdaptiveAvgPool2d((1, 1)), nn.Flatten()
            )
            self.fc = None  # Will be dynamically set
        def forward(self, x):
            x = self.net(x)
            if self.fc is None:
                self.fc = nn.Linear(x.shape[1], 10)
            return self.fc(x)
    x = torch.randn(2, 1, 64, 64)
    densenet = DenseNet()
    out = densenet(x)
    print("DenseNetè¾“å‡ºå½¢çŠ¶:", out.shape)  # [2, 10]

    # 9. æ‰¹é‡å½’ä¸€åŒ–
    bn = nn.BatchNorm2d(6)
    # åŸç†ï¼šå¯¹æ¯ä¸ªé€šé“åšå½’ä¸€åŒ–ï¼Œç¼“è§£æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸ï¼ŒåŠ é€Ÿæ”¶æ•›
    x = torch.randn(4, 6, 10, 10)
    out = bn(x)
    print("BatchNormè¾“å‡ºå½¢çŠ¶:", out.shape)  # [4, 6, 10, 10]

    # 10. æ®‹å·®å—
    class Residual(nn.Module):
        """
        æ®‹å·®å—ï¼ˆResNetæ ¸å¿ƒå•å…ƒï¼‰
        åŸç†ï¼šæ’ç­‰æ˜ å°„+å·ç§¯ï¼Œä¾¿äºæ¢¯åº¦æµåŠ¨ï¼Œè§£å†³æ·±å±‚ç½‘ç»œé€€åŒ–
        """
        def __init__(self, in_channels, out_channels, use_1x1conv=False, stride=1):
            super().__init__()
            self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, stride=stride)
            self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)
            self.bn1 = nn.BatchNorm2d(out_channels)
            self.bn2 = nn.BatchNorm2d(out_channels)
            if use_1x1conv:
                self.conv3 = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride)
            else:
                self.conv3 = None
        def forward(self, X):
            Y = torch.relu(self.bn1(self.conv1(X)))
            Y = self.bn2(self.conv2(Y))
            if self.conv3:
                X = self.conv3(X)
            return torch.relu(Y + X)
    x = torch.randn(2, 3, 16, 16)
    block = Residual(3, 3)
    out = block(x)
    print("æ®‹å·®å—è¾“å‡ºå½¢çŠ¶:", out.shape)  # [2, 3, 16, 16]

    print("\nã€æ€»ç»“ã€‘")
    print("- æ¯ä¸ªæ¨¡å—éƒ½å¯ç‹¬ç«‹æµ‹è¯•å’Œç»„åˆï¼Œé€‚åˆæ­å»ºå„ç±» CNN ç½‘ç»œã€‚")
    print("- æ¨èå·¥ä½œæµç¨‹ï¼šåŸºç¡€å·ç§¯/æ± åŒ– â†’ ç»„åˆæ·±å±‚ç»“æ„ï¼ˆå¦‚LeNet/AlexNet/VGG/GoogLeNet/DenseNet/ResNetï¼‰â†’ åŠ å…¥BatchNorm/æ®‹å·®å— â†’ è®­ç»ƒä¸è¯„ä¼°ã€‚")
    print("- è®¾è®¡èƒŒåçš„æ ¸å¿ƒæ€æƒ³ï¼šç©ºé—´ç‰¹å¾æå–ã€å‚æ•°é«˜æ•ˆã€æ¢¯åº¦æµåŠ¨ã€å½’ä¸€åŒ–æå‡è®­ç»ƒç¨³å®šæ€§ã€å¤šå°ºåº¦ç‰¹å¾èåˆã€ç‰¹å¾å¤ç”¨ã€‚")

    # æ®‹å·®ç½‘ç»œçš„å¿…è¦æ€§éªŒè¯ï¼šè§£å†³æ·±åº¦ç½‘ç»œçš„é€€åŒ–é—®é¢˜
    print("\n========== æ®‹å·®ç½‘ç»œæ·±åº¦å¯¹æ¯”éªŒè¯ ==========")
    
    # æ·±å±‚æ™®é€šç½‘ç»œ vs æ®‹å·®ç½‘ç»œ
    class PlainNet(nn.Module):
        """æ·±å±‚æ™®é€šç½‘ç»œï¼ˆæ— æ®‹å·®è¿æ¥ï¼‰"""
        def __init__(self, depth=20):
            super().__init__()
            layers = [nn.Conv2d(3, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU()]
            for _ in range(depth-2):
                layers.extend([
                    nn.Conv2d(64, 64, 3, padding=1), 
                    nn.BatchNorm2d(64), 
                    nn.ReLU()
                ])
            layers.append(nn.Conv2d(64, 10, 1))
            self.network = nn.Sequential(*layers)
            
        def forward(self, x):
            return self.network(x)
    
    class ResNet18Simplified(nn.Module):
        """ç®€åŒ–ç‰ˆResNet-18ï¼Œå±•ç¤ºæ®‹å·®è¿æ¥çš„ä½œç”¨"""
        def __init__(self):
            super().__init__()
            self.conv1 = nn.Conv2d(3, 64, 7, stride=2, padding=3)
            self.bn1 = nn.BatchNorm2d(64)
            self.relu = nn.ReLU(inplace=True)
            self.maxpool = nn.MaxPool2d(3, stride=2, padding=1)
            
            # æ®‹å·®å—
            self.layer1 = self._make_layer(64, 64, 2)
            self.layer2 = self._make_layer(64, 128, 2, stride=2)
            self.layer3 = self._make_layer(128, 256, 2, stride=2)
            self.layer4 = self._make_layer(256, 512, 2, stride=2)
            
            self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
            self.fc = nn.Linear(512, 10)
            
        def _make_layer(self, in_channels, out_channels, blocks, stride=1):
            layers = []
            # ç¬¬ä¸€ä¸ªæ®‹å·®å—å¯èƒ½éœ€è¦ä¸‹é‡‡æ ·
            layers.append(Residual(in_channels, out_channels, 
                                 use_1x1conv=(stride != 1 or in_channels != out_channels), 
                                 stride=stride))
            # åç»­æ®‹å·®å—
            for _ in range(1, blocks):
                layers.append(Residual(out_channels, out_channels))
            return nn.Sequential(*layers)
            
        def forward(self, x):
            x = self.conv1(x)
            x = self.bn1(x)
            x = self.relu(x)
            x = self.maxpool(x)
            
            x = self.layer1(x)
            x = self.layer2(x)
            x = self.layer3(x)
            x = self.layer4(x)
            
            x = self.avgpool(x)
            x = torch.flatten(x, 1)
            x = self.fc(x)
            return x
    
    # ç½‘ç»œå¤æ‚åº¦å¯¹æ¯”
    def count_parameters(model):
        return sum(p.numel() for p in model.parameters() if p.requires_grad)
    
    plain_net = PlainNet(depth=18)
    resnet = ResNet18Simplified()
    
    print(f"æ™®é€šæ·±å±‚ç½‘ç»œå‚æ•°é‡: {count_parameters(plain_net):,}")
    print(f"ResNet-18å‚æ•°é‡: {count_parameters(resnet):,}")
    
    # æ¢¯åº¦æµåŠ¨éªŒè¯
    x = torch.randn(2, 3, 224, 224)
    y_plain = plain_net(x)
    y_res = resnet(x)
    
    print(f"æ™®é€šç½‘ç»œè¾“å‡ºå½¢çŠ¶: {y_plain.shape}")
    print(f"ResNetè¾“å‡ºå½¢çŠ¶: {y_res.shape}")
    
    # æ¨¡æ‹Ÿè®­ç»ƒä¸­çš„æ¢¯åº¦æƒ…å†µ
    criterion = nn.CrossEntropyLoss()
    target = torch.randint(0, 10, (2,))
    
    # æ·»åŠ å…¨å±€å¹³å‡æ± åŒ–å¤„ç†æ™®é€šç½‘ç»œçš„è¾“å‡º
    y_plain_pooled = nn.functional.adaptive_avg_pool2d(y_plain, (1, 1)).view(y_plain.shape[0], -1)
    
    # æ™®é€šç½‘ç»œæ¢¯åº¦
    loss_plain = criterion(y_plain_pooled, target)
    loss_plain.backward()
    plain_grads = [p.grad.norm().item() for p in plain_net.parameters() if p.grad is not None]
    
    # ResNetæ¢¯åº¦ 
    loss_res = criterion(y_res, target)
    loss_res.backward()
    res_grads = [p.grad.norm().item() for p in resnet.parameters() if p.grad is not None]
    
    print(f"æ™®é€šç½‘ç»œå¹³å‡æ¢¯åº¦èŒƒæ•°: {np.mean(plain_grads):.6f}")
    print(f"ResNetå¹³å‡æ¢¯åº¦èŒƒæ•°: {np.mean(res_grads):.6f}")
    print("â€» ResNetçš„æ¢¯åº¦æ›´ç¨³å®šï¼Œæœ‰åˆ©äºæ·±å±‚ç½‘ç»œè®­ç»ƒ")
    
    # 8. ç°ä»£CNNæŠ€æœ¯ç»¼åˆåº”ç”¨
    print("\n========== ç°ä»£CNNæŠ€æœ¯é›†æˆåº”ç”¨ ==========")
    
    class ModernCNN(nn.Module):
        """
        ç°ä»£CNNé›†æˆï¼šæ‰¹å½’ä¸€åŒ–+æ®‹å·®è¿æ¥+æ³¨æ„åŠ›æœºåˆ¶+è‡ªé€‚åº”æ± åŒ–
        é€‚ç”¨åœºæ™¯ï¼šå›¾åƒåˆ†ç±»ã€ç‰¹å¾æå–ã€è¿ç§»å­¦ä¹ 
        """
        def __init__(self, num_classes=10):
            super().__init__()
            # åˆå§‹å·ç§¯å±‚
            self.stem = nn.Sequential(
                nn.Conv2d(3, 64, 7, stride=2, padding=3),
                nn.BatchNorm2d(64),
                nn.ReLU(inplace=True),
                nn.MaxPool2d(3, stride=2, padding=1)
            )
            
            # æ®‹å·®å±‚
            self.layer1 = self._make_layer(64, 64, 2)
            self.layer2 = self._make_layer(64, 128, 2, stride=2)
            self.layer3 = self._make_layer(128, 256, 2, stride=2)
            self.layer4 = self._make_layer(256, 512, 2, stride=2)
            
            # å…¨å±€è‡ªé€‚åº”å¹³å‡æ± åŒ–ï¼ˆæ›¿ä»£å…¨è¿æ¥å±‚ï¼Œå‡å°‘å‚æ•°ï¼‰
            self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
            
            # åˆ†ç±»å¤´
            self.classifier = nn.Sequential(
                nn.Dropout(0.5),
                nn.Linear(512, num_classes)
            )
            
        def _make_layer(self, in_channels, out_channels, blocks, stride=1):
            layers = []
            layers.append(Residual(in_channels, out_channels, 
                                 use_1x1conv=(stride != 1 or in_channels != out_channels), 
                                 stride=stride))
            for _ in range(1, blocks):
                layers.append(Residual(out_channels, out_channels))
            return nn.Sequential(*layers)
            
        def forward(self, x):
            x = self.stem(x)
            
            x = self.layer1(x)
            x = self.layer2(x)
            x = self.layer3(x)
            x = self.layer4(x)
            
            x = self.avgpool(x)
            x = torch.flatten(x, 1)
            x = self.classifier(x)
            return x
    
    modern_cnn = ModernCNN(num_classes=10)
    x = torch.randn(4, 3, 224, 224)
    out = modern_cnn(x)
    print(f"ç°ä»£CNNè¾“å‡ºå½¢çŠ¶: {out.shape}")  # [4, 10]
    print(f"ç°ä»£CNNå‚æ•°é‡: {count_parameters(modern_cnn):,}")
    
    # 9. CNNæ€§èƒ½ä¼˜åŒ–ä¸è°ƒè¯•æŠ€å·§
    print("\n========== CNNæ€§èƒ½ä¼˜åŒ–ä¸è°ƒè¯•æŠ€å·§ ==========")
    
    # ç‰¹å¾å›¾å¯è§†åŒ–ï¼ˆç”¨äºè°ƒè¯•å’Œç†è§£ï¼‰
    def visualize_feature_maps(model, x, layer_name="layer1"):
        """
        ç‰¹å¾å›¾å¯è§†åŒ–ï¼šå¸®åŠ©ç†è§£CNNå­¦åˆ°çš„ç‰¹å¾
        åœ¨å®é™…è°ƒè¯•ä¸­ï¼Œå¯ä»¥æŸ¥çœ‹ä¸­é—´å±‚è¾“å‡ºï¼ŒéªŒè¯ç½‘ç»œæ˜¯å¦å­¦åˆ°æœ‰æ„ä¹‰çš„ç‰¹å¾
        """
        activation = {}
        def get_activation(name):
            def hook(model, input, output):
                activation[name] = output.detach()
            return hook
        
        # æ³¨å†Œé’©å­å‡½æ•°
        getattr(model, layer_name).register_forward_hook(get_activation(layer_name))
        
        # å‰å‘ä¼ æ’­
        _ = model(x)
        
        # è·å–ç‰¹å¾å›¾
        feature_map = activation[layer_name]
        print(f"{layer_name}ç‰¹å¾å›¾å½¢çŠ¶: {feature_map.shape}")
        
        # è®¡ç®—ç‰¹å¾å›¾ç»Ÿè®¡ä¿¡æ¯
        mean_activation = feature_map.mean().item()
        std_activation = feature_map.std().item()
        print(f"ç‰¹å¾å›¾å‡å€¼: {mean_activation:.4f}, æ ‡å‡†å·®: {std_activation:.4f}")
        
        return feature_map
    
    # å¯è§†åŒ–ç°ä»£CNNçš„ç‰¹å¾
    x_vis = torch.randn(1, 3, 224, 224)
    feature_map = visualize_feature_maps(modern_cnn, x_vis, "layer2")
    
    # æ¨¡å‹æ•ˆç‡åˆ†æ
    def analyze_model_efficiency(model, input_size=(1, 3, 224, 224)):
        """åˆ†ææ¨¡å‹è®¡ç®—æ•ˆç‡ï¼šFLOPsã€å‚æ•°é‡ã€å†…å­˜å ç”¨"""
        x = torch.randn(*input_size)
        
        # å‚æ•°é‡
        total_params = count_parameters(model)
        
        # æ¨¡å‹å¤§å°ï¼ˆMBï¼‰
        model_size = sum(p.numel() * p.element_size() for p in model.parameters()) / (1024**2)
        
        # å‰å‘ä¼ æ’­æ—¶é—´æµ‹è¯•
        model.eval()
        import time
        start_time = time.time()
        with torch.no_grad():
            for _ in range(100):
                _ = model(x)
        avg_time = (time.time() - start_time) / 100
        
        print(f"å‚æ•°é‡: {total_params:,}")
        print(f"æ¨¡å‹å¤§å°: {model_size:.2f} MB")
        print(f"å¹³å‡æ¨ç†æ—¶é—´: {avg_time*1000:.2f} ms")
        
        return total_params, model_size, avg_time
    
    # æ•ˆç‡å¯¹æ¯”
    print("\n--- LeNetæ•ˆç‡åˆ†æ ---")
    analyze_model_efficiency(LeNet(), (1, 1, 28, 28))
    
    print("\n--- ModernCNNæ•ˆç‡åˆ†æ ---")
    analyze_model_efficiency(modern_cnn, (1, 3, 224, 224))
    
    # 10. è¿ç§»å­¦ä¹ ä¸å®é™…åº”ç”¨
    print("\n========== è¿ç§»å­¦ä¹ ä¸å®é™…åº”ç”¨ ==========")
    
    def create_transfer_model(num_classes=2, freeze_backbone=True):
        """
        åˆ›å»ºè¿ç§»å­¦ä¹ æ¨¡å‹ï¼šåŸºäºé¢„è®­ç»ƒCNNè¿›è¡Œå¾®è°ƒ
        é€‚ç”¨åœºæ™¯ï¼šæ•°æ®é‡è¾ƒå°çš„å›¾åƒåˆ†ç±»ä»»åŠ¡
        """
        # ä½¿ç”¨ç°æœ‰çš„ModernCNNä½œä¸ºbackbone
        backbone = ModernCNN(num_classes=1000)  # å‡è®¾åœ¨ImageNetä¸Šé¢„è®­ç»ƒ
        
        # å†»ç»“backboneå‚æ•°ï¼ˆå¯é€‰ï¼‰
        if freeze_backbone:
            for param in backbone.parameters():
                param.requires_grad = False
        
        # æ›¿æ¢åˆ†ç±»å¤´
        backbone.classifier = nn.Sequential(
            nn.Dropout(0.5),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(256, num_classes)
        )
        
        return backbone
    
    # åˆ›å»ºäºŒåˆ†ç±»è¿ç§»å­¦ä¹ æ¨¡å‹
    transfer_model = create_transfer_model(num_classes=2, freeze_backbone=True)
    x = torch.randn(2, 3, 224, 224)
    out = transfer_model(x)
    print(f"è¿ç§»å­¦ä¹ æ¨¡å‹è¾“å‡ºå½¢çŠ¶: {out.shape}")  # [2, 2]
    
    # ç»Ÿè®¡å¯è®­ç»ƒå‚æ•°
    trainable_params = sum(p.numel() for p in transfer_model.parameters() if p.requires_grad)
    total_params = sum(p.numel() for p in transfer_model.parameters())
    print(f"æ€»å‚æ•°é‡: {total_params:,}")
    print(f"å¯è®­ç»ƒå‚æ•°é‡: {trainable_params:,}")
    print(f"å‚æ•°å†»ç»“æ¯”ä¾‹: {(1 - trainable_params/total_params)*100:.1f}%")
    
    # 11. CNNè®­ç»ƒæœ€ä½³å®è·µ
    print("\n========== CNNè®­ç»ƒæœ€ä½³å®è·µ ==========")
    
    def cnn_training_demo():
        """
        CNNè®­ç»ƒå®Œæ•´æµç¨‹æ¼”ç¤ºï¼šæ•°æ®å¢å¼º+å­¦ä¹ ç‡è°ƒåº¦+æ—©åœ
        å±•ç¤ºå·¥ç¨‹ä¸­çš„å®é™…è®­ç»ƒæŠ€å·§
        """
        # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®é›†
        def create_dummy_dataset(num_samples=1000, num_classes=10):
            X = torch.randn(num_samples, 3, 32, 32)
            y = torch.randint(0, num_classes, (num_samples,))
            return X, y
        
        X_train, y_train = create_dummy_dataset(800, 10)
        X_val, y_val = create_dummy_dataset(200, 10)
        
        # æ•°æ®å¢å¼ºï¼ˆæ¨¡æ‹Ÿï¼‰
        print("æ•°æ®å¢å¼ºç­–ç•¥ï¼šéšæœºç¿»è½¬ã€æ—‹è½¬ã€ç¼©æ”¾ã€é¢œè‰²å˜æ¢")
        
        # åˆ›å»ºé€‚åˆå°å›¾åƒçš„CNN
        class CIFAR_CNN(nn.Module):
            def __init__(self, num_classes=10):
                super().__init__()
                self.features = nn.Sequential(
                    nn.Conv2d(3, 32, 3, padding=1), nn.BatchNorm2d(32), nn.ReLU(),
                    nn.Conv2d(32, 32, 3, padding=1), nn.BatchNorm2d(32), nn.ReLU(),
                    nn.MaxPool2d(2, 2),
                    
                    nn.Conv2d(32, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(),
                    nn.Conv2d(64, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(),
                    nn.MaxPool2d(2, 2),
                    
                    nn.Conv2d(64, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(),
                    nn.AdaptiveAvgPool2d((4, 4))
                )
                self.classifier = nn.Sequential(
                    nn.Dropout(0.5),
                    nn.Linear(128 * 4 * 4, 256),
                    nn.ReLU(),
                    nn.Dropout(0.3),
                    nn.Linear(256, num_classes)
                )
            
            def forward(self, x):
                x = self.features(x)
                x = x.view(x.size(0), -1)
                x = self.classifier(x)
                return x
        
        model = CIFAR_CNN(10)
        
        # ä¼˜åŒ–å™¨è®¾ç½®
        optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)
        
        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)
        
        criterion = nn.CrossEntropyLoss()
        
        print(f"CIFAR-style CNNå‚æ•°é‡: {count_parameters(model):,}")
        
        # æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹ï¼ˆç®€åŒ–ç‰ˆï¼‰
        model.train()
        train_loss = criterion(model(X_train[:32]), y_train[:32])
        print(f"åˆå§‹è®­ç»ƒæŸå¤±: {train_loss.item():.4f}")
        
        # éªŒè¯è¿‡ç¨‹
        model.eval()
        with torch.no_grad():
            val_out = model(X_val[:32])
            val_loss = criterion(val_out, y_val[:32])
            val_acc = (val_out.argmax(dim=1) == y_val[:32]).float().mean()
        
        print(f"éªŒè¯æŸå¤±: {val_loss.item():.4f}")
        print(f"éªŒè¯å‡†ç¡®ç‡: {val_acc.item():.3f}")
        
        return model
    
    trained_model = cnn_training_demo()
    
    # 12. æ€»ç»“ä¸å±•æœ›
    print("\n========== CNNæŠ€æœ¯æ€»ç»“ä¸å±•æœ› ==========")
    print("""
    ã€æ ¸å¿ƒæŠ€æœ¯è¦ç‚¹ã€‘
    1. å·ç§¯å±‚ï¼šå±€éƒ¨è¿æ¥ã€æƒé‡å…±äº«ï¼Œé€‚åˆå¤„ç†å…·æœ‰ç©ºé—´ç»“æ„çš„æ•°æ®
    2. æ± åŒ–å±‚ï¼šé™é‡‡æ ·ã€å¢å¼ºä¸å˜æ€§ï¼Œå‡å°‘è®¡ç®—é‡å’Œè¿‡æ‹Ÿåˆé£é™©
    3. æ‰¹å½’ä¸€åŒ–ï¼šç¨³å®šè®­ç»ƒã€åŠ é€Ÿæ”¶æ•›ï¼Œç°ä»£CNNçš„æ ‡å‡†ç»„ä»¶
    4. æ®‹å·®è¿æ¥ï¼šè§£å†³æ·±åº¦ç½‘ç»œè®­ç»ƒå›°éš¾ï¼Œå®ç°æ›´æ·±çš„ç½‘ç»œç»“æ„
    5. æ³¨æ„åŠ›æœºåˆ¶ï¼šé‡ç‚¹å…³æ³¨é‡è¦ç‰¹å¾ï¼Œæå‡æ¨¡å‹è¡¨è¾¾èƒ½åŠ›
    
    ã€æ¶æ„æ¼”è¿›å†ç¨‹ã€‘
    LeNet(1998) â†’ AlexNet(2012) â†’ VGG(2014) â†’ ResNet(2015) â†’ DenseNet(2017) â†’ Vision Transformer(2020)
    
    ã€å·¥ç¨‹å®è·µå»ºè®®ã€‘
    1. æ•°æ®é¢„å¤„ç†ï¼šæ ‡å‡†åŒ–ã€æ•°æ®å¢å¼ºæå‡æ³›åŒ–èƒ½åŠ›
    2. ç½‘ç»œè®¾è®¡ï¼šä»ç®€å•å¼€å§‹ï¼Œé€æ­¥åŠ æ·±ï¼Œæ³¨æ„è®¡ç®—æ•ˆç‡
    3. è®­ç»ƒæŠ€å·§ï¼šå­¦ä¹ ç‡è°ƒåº¦ã€æ—©åœã€æ¢¯åº¦è£å‰ªé˜²æ­¢è¿‡æ‹Ÿåˆ
    4. è°ƒè¯•æ–¹æ³•ï¼šç‰¹å¾å›¾å¯è§†åŒ–ã€æ¢¯åº¦ç›‘æ§ã€æŸå¤±æ›²çº¿åˆ†æ
    5. éƒ¨ç½²ä¼˜åŒ–ï¼šæ¨¡å‹å‹ç¼©ã€é‡åŒ–ã€è’¸é¦é€‚åº”å®é™…åº”ç”¨
    
    ã€æœªæ¥å‘å±•è¶‹åŠ¿ã€‘
    1. æ›´é«˜æ•ˆçš„æ¶æ„ï¼šMobileNetã€EfficientNetç­‰è½»é‡åŒ–ç½‘ç»œ
    2. è‡ªåŠ¨ç½‘ç»œæœç´¢ï¼šNASæŠ€æœ¯è‡ªåŠ¨è®¾è®¡æœ€ä¼˜æ¶æ„
    3. å¤šæ¨¡æ€èåˆï¼šCNN+Transformerç»“åˆå¤„ç†å¤æ‚ä»»åŠ¡
    4. è¾¹ç¼˜è®¡ç®—ï¼šé’ˆå¯¹ç§»åŠ¨è®¾å¤‡çš„æ¨¡å‹å‹ç¼©å’ŒåŠ é€Ÿ
    """)
    
    print("\nğŸ¯ CNNå­¦ä¹ å»ºè®®ï¼š")
    print("1. ç†è§£å·ç§¯çš„æ•°å­¦åŸç†å’Œå‡ ä½•æ„ä¹‰")
    print("2. åŠ¨æ‰‹å®ç°ç»å…¸ç½‘ç»œï¼ŒåŠ æ·±ç†è§£")
    print("3. é€šè¿‡å¯è§†åŒ–è§‚å¯Ÿç½‘ç»œå­¦åˆ°çš„ç‰¹å¾")
    print("4. åœ¨å®é™…é¡¹ç›®ä¸­åº”ç”¨ï¼Œç§¯ç´¯è°ƒå‚ç»éªŒ")
    print("5. å…³æ³¨æœ€æ–°ç ”ç©¶è¿›å±•ï¼Œä¿æŒæŠ€æœ¯æ•æ„Ÿåº¦") 